{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">LOGISTIC REGRESSION ON LARGE DATASET - SIDES </h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#ff6347\">Data</h3>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <b><big>Format of the Data (train_data.csc)</big></b>\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "\n",
    "\n",
    "<style>\n",
    "table {\n",
    "  font-size: 12px;\n",
    "}\n",
    "</style>\n",
    "\n",
    "| question | answer_option | n_correct_answer_option | n_answer_option | type | specialty                      | specialty_id | student | result | test   | date     | time | date_time                  |\n",
    "|----------|---------------|------------------------|-----------------|------|--------------------------------|--------------|---------|--------|--------|----------|------|----------------------------|\n",
    "| 3141719  | QMA           | 1                      | 5               | DP   | endocrinology_metabolism_nutrition | 7            | 191572  | 1      | 2703967 | 20210216 | 1359 | 2021-02-16T00:13:59Z |\n",
    "| 3160174  | QMA           | 1                      | 5               | DP   | endocrinology_metabolism_nutrition | 7            | 191572  | 1      | 2703967 | 20210216 | 1359 | 2021-02-16T00:13:59Z |\n",
    "| 3160183  | QMA           | 2                      | 5               | DP   | endocrinology_metabolism_nutrition | 7            | 191572  | 1      | 2703967 | 20210216 | 1359 | 2021-02-16T00:13:59Z |\n",
    "| 3160244  | QMA           | 1                      | 5               | DP   | endocrinology_metabolism_nutrition | 7            | 191572  | 0      | 2703967 | 20210216 | 1359 | 2021-02-16T00:13:59Z |\n",
    "| 3160261  | QMA           | 1                      | 5               | DP   | endocrinology_metabolism_nutrition | 7            | 191572  | 1      | 2703967 | 20210216 | 1359 | 2021-02-16T00:13:59Z |\n",
    "\n",
    "\n",
    "#### Properties of the Table:\n",
    "\n",
    "1. The table is organized into rows and columns.\n",
    "2. Each row represents a specific question along with its associated data.\n",
    "3. Each column represents a specific attribute or property of the question.\n",
    "4. The data is structured in a tabular form for easy readability and comparison.\n",
    "5. The table includes numeric, text, and date/time data types.\n",
    "6. The first row is the header row, which provides the names of each column for clarity.\n",
    "\n",
    "####  Column Descriptions:\n",
    "\n",
    "| Column Name              | Description                                                                                         |\n",
    "| ------------------------ | --------------------------------------------------------------------------------------------------- |\n",
    "| question                 | The question ID.                                                                                    |\n",
    "| answer_option            | Either QMA or QMB. QMA (question multiple answer) means that multiple answers are possible. QUA (question unique answer) means that only one answer is possible. |\n",
    "| n_correct_answer_option  | The number of correct answers.                                                                      |\n",
    "| n_answer_option          | The number of options.                                                                              |\n",
    "| type                     | The type of question. QI (question isole), DP (dossier progressif), or LCA (lecture critique d'article). These are SIDES specific question types. |\n",
    "| specialty                | The specialty of the question. If the question is tagged with multiple specialties, they are separated by a + sign. |\n",
    "| spec_id                  | The specialty ID. This is a numeric value that corresponds to the specialty.                       |\n",
    "| student                  | The student ID.                                                                                     |\n",
    "| result                   | The result of the student on the question. 0 means incorrect, 1 means correct. (don't need to be binary, it will be done in the code) |\n",
    "| test                     | The test ID.                                                                                        |\n",
    "| date                     | The date of the test.                                                                               |\n",
    "| time                     | The time of the test.                                                                               |\n",
    "| date_time                | The date and time of the test.                                                                      |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#ff6347\">Import Necessary Libraries</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from prepare_data_SIDES.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Standard Library Imports\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import logging\n",
    "import argparse\n",
    "import glob\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Third-Party Library Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import sparse\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import OneHotEncoder, normalize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, log_loss, mean_squared_error\n",
    "\n",
    "# Project-Specific Imports\n",
    "import json\n",
    "import dataio\n",
    "import utils.this_queue as OurQueue\n",
    "\n",
    "# Additional Imports\n",
    "import sys\n",
    "\n",
    "from scipy.sparse import load_npz, hstack, csr_matrix, csr_matrix, find\n",
    "import joblib\n",
    "\n",
    "# import prepare_data as pdr\n",
    "import import_ipynb\n",
    "import prepare_data_SIDES as sides_pdr\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#ff6347\">Parameters</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main path that includes all the functions and the main code\n",
    "main_path = 'c:/Users/Ghislaine/Desktop/knowledge_traing_EloRating'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dettermine course_id\n",
    "semester = '2019-2020'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "configurations = {\n",
    "    '2019-2020' : {\n",
    "    'folder' : 'data/sides/',\n",
    "    'education_year' : '2019-2020',\n",
    "    'min_interactions_per_user' : 100,\n",
    "    'min_answer_per_question' : 100,\n",
    "    'kc_column' : 'specialty', \n",
    "    'train_file' : 'train_data.csv',\n",
    "    'test_file' : 'test_data.csv',\n",
    "    'already_preprocessed':False,\n",
    "    'user_skill_item_already_encoded':False\n",
    "    },\n",
    "    \n",
    "    '2020-2021' : {\n",
    "    'folder' : 'data/sides/',\n",
    "    'education_year' : '2020-2021',\n",
    "    'min_interactions_per_user' : 100,\n",
    "    'min_answer_per_question' : 100,\n",
    "    'kc_column' : 'specialty', \n",
    "    'train_file' : 'train_data.csv',\n",
    "    'test_file' : 'test_data.csv',\n",
    "    'already_preprocessed':False,\n",
    "    'user_skill_item_already_encoded':True\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding & Logistic Regression Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Options():\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Encoding\n",
    "        self.users = True\n",
    "        self.user_skill_together= True\n",
    "        self.items = True\n",
    "        self.skills = True\n",
    "        self.file_splits = 1\n",
    "        self.by_spec = False\n",
    "        self.spec_difficulty = True\n",
    "        self.weighted_encoding = True\n",
    "        \n",
    "        # Logistic Regression\n",
    "        self.iter = 300\n",
    "        self.C = 1.0 #1e-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = Options()\n",
    "# print an error message if the user_skill_together is set to True but skills or users are set to False\n",
    "if options.user_skill_together and (not options.skills or not options.users):\n",
    "    raise ValueError('user_skill_together is set to True but skills or users are set to False. Please set user_skill_together to False or set skills and users to True.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "folder = configurations[semester]['folder']\n",
    "education_year = configurations[semester]['education_year']\n",
    "min_interactions_per_user = configurations[semester]['min_interactions_per_user']\n",
    "min_answer_per_question = configurations[semester]['min_answer_per_question']\n",
    "kc_column = configurations[semester]['kc_column']\n",
    "train_file = configurations[semester]['train_file']\n",
    "test_file = configurations[semester]['test_file']\n",
    "already_preprocessed=configurations[semester]['already_preprocessed']\n",
    "user_skill_item_already_encoded=configurations[semester]['user_skill_item_already_encoded']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save results seperately for seperate versions of the model & save features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features have been written to the file: features.txt in the  data/sides//2019-2020/result_logreg_user_skill_together-spec_difficulty-items-weighted_encoding/\n"
     ]
    }
   ],
   "source": [
    "all_features = ['users', 'user_skill_together','spec_difficulty' ,'items', 'skills', 'weighted_encoding']\n",
    "active_features = [features for features in all_features if vars(options)[features]]\n",
    "\n",
    "# Check if 'user_skill_together', 'users', and 'skills' are all active features\n",
    "if 'user_skill_together' in active_features and 'users' in active_features and 'skills' in active_features:\n",
    "    active_features.remove('users')\n",
    "    active_features.remove('skills')\n",
    "\n",
    "additional_suffix = '-'.join(active_features)\n",
    "\n",
    "EXPERIMENT_FOLDER = folder + '/' + education_year + '/result_logreg_' + additional_suffix + '/'\n",
    "\n",
    "dataio.prepare_folder(EXPERIMENT_FOLDER)\n",
    "\n",
    "# Save the configurations and parameters to a TXT file\n",
    "\n",
    "# Combine configurations and hyper_params into a single dictionary\n",
    "# Convert Options object to a dictionary\n",
    "options_dict = options.__dict__\n",
    "combined_dict = {\n",
    "    'configurations': configurations[semester],\n",
    "    'options': options_dict\n",
    "}\n",
    "\n",
    "# Write the configurations dictionary to the TXT file\n",
    "with open(EXPERIMENT_FOLDER + 'features.txt', 'w') as file:\n",
    "    json.dump(combined_dict, file, indent=4)\n",
    "\n",
    "print(\"Features have been written to the file: features.txt in the \", EXPERIMENT_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\" style=\"color:blue\">3 STEPS FOR LOGISTIC REGRESSION</h2>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#228B22\">Step-1: Preprocessing the Data</h3>\n",
    "\n",
    "\n",
    "Preprocessing includes these major steps:\n",
    "<small>\n",
    "1. Removing duplicates\n",
    "2. Removing Rows with Empty Values for KC (Knowledge Component)\n",
    "3. Removing Rows with Empty Values for n_correct_options\n",
    "4. Transforming Non-Binary Scores\n",
    "5. Removing Users with Insufficient Interactions\n",
    "6. Removing Items with Insufficient Interactions\n",
    "7. Creating Variables and Transforming IDs: Variables are created, and user and item IDs are transformed to numeric values.\n",
    "8. Renaming Questions/Skills: The questions/skills in the \"item_skills\" data are renamed using the item IDs.\n",
    "9. Creating Q-matrix: A Q-matrix is created, where each row represents a question and each column represents a skill.\n",
    "10. Saving the Data: The preprocessed data is saved as a CSV file.\n",
    "\n",
    "</small>\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <b><big>Preprocessed data</big></b>\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "\n",
    "\n",
    "<style>\n",
    "table {\n",
    "  font-size: 12px;\n",
    "}\n",
    "</style>\n",
    "\n",
    "| user_id | item_id | n_options | answer_type | timestamp | correct | kc_id                           | group |\n",
    "|---------|---------|-----------|-------------|-----------|---------|---------------------------------|-------|\n",
    "| 2       | 0       | 5         | QMA         | 0         | 1       | endocrinology_metabolism_nutrition | 0     |\n",
    "| 2       | 1       | 5         | QMA         | 0         | 1       | endocrinology_metabolism_nutrition | 0     |\n",
    "| 2       | 2       | 5         | QMA         | 0         | 1       | endocrinology_metabolism_nutrition | 0     |\n",
    "| 2       | 3       | 5         | QMA         | 0         | 0       | endocrinology_metabolism_nutrition | 0     |\n",
    "| 2       | 4       | 5         | QMA         | 0         | 1       | endocrinology_metabolism_nutrition | 0     |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data not preprocessed. Preprocessing data...\n",
      "Opened SIDES train data. Output: 40618623 samples.\n",
      "Opened SIDES test data. Output: 10154656 samples.\n",
      "Removed 0 duplicated samples.\n",
      "Removed 0 samples with NaN skills.\n",
      "Removed 0 samples with NA answer_type.\n",
      "Removed 0 samples with non-binary outcomes.\n",
      "Removed 13231806 samples \n",
      "(users with less than 100 interactions).\n",
      "Removed 5119382 samples \n",
      "(questions with less than 100 answers).\n",
      "Computed q-matrix. Shape: (97647, 31).\n",
      "Data preprocessing done. Final output: 32422091 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:204: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "<string>:210: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# if data already preprocessed and  if preprocessed_data.csv exists in folder+'/'+ education_year+\"/processed/\n",
    "if already_preprocessed and os.path.exists(folder+'/'+ education_year+\"/processed/preprocessed_data.csv\"):\n",
    "    # print message\n",
    "    print(\"Data already preprocessed. Reading preprocessed data...\")\n",
    "    # read csv file preprocessed_data.csv\n",
    "    data= pd.read_csv(folder+'/'+ education_year+\"/processed/preprocessed_data.csv\")\n",
    "    # read npz file q_mat.npz\n",
    "    q_mat =q_mat = sparse.load_npz(folder + '/' + education_year + \"/processed/q_mat.npz\").toarray()\n",
    "    # # reaad json file config.json\n",
    "    # with open(folder+'/'+ education_year+\"/processed/config.json\") as f:\n",
    "    #     config = json.load(f)\n",
    "        \n",
    "    \n",
    "else:\n",
    "    print(\"Data not preprocessed. Preprocessing data...\")\n",
    "    warnings.filterwarnings(action='once')\n",
    "    # processing the row data made available by KDD organisers\n",
    "    data, q_mat, listOfKC, dict_of_kc, train_set, test_set,skill_names_ids_map_df = sides_pdr.prepare_sides(folder, education_year, \\\n",
    "                                                                    train_file, \\\n",
    "                                                                    test_file,\\\n",
    "                                                                        kc_column, min_interactions_per_user, min_answer_per_question,\\\n",
    "                                                                        True, True, True,True)\n",
    "            \n",
    "    # delete unnecessary dataframes\n",
    "    del train_set\n",
    "    del test_set\n",
    "\n",
    "# only keep the columns that are needed for the logistic reression model\n",
    "data = data[['user_id', 'item_id', 'timestamp', 'correct', 'group']]\n",
    "# sort the data by user_id and timestamp\n",
    "data.sort_values(by=[\"user_id\",\"timestamp\"], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique students is: 24757\n",
      "Number of unique questions is: 97647\n",
      "Number of unique specialty in data:  31\n"
     ]
    }
   ],
   "source": [
    "# number of students\n",
    "uSize = data['user_id'].nunique()\n",
    "print(\"Number of unique students is:\", uSize)\n",
    "\n",
    "qSize = data['item_id'].nunique()\n",
    "print(\"Number of unique questions is:\", qSize)\n",
    "\n",
    "tSize = q_mat.shape[1]\n",
    "print(\"Number of unique specialty in data: \", tSize )\n",
    "\n",
    "#define the number of attempts per student\n",
    "attempt_counter_question = np.zeros(qSize)\n",
    "\n",
    "# from data select only group==0\n",
    "data = data[data['group']==0]\n",
    "# number of attempts per question\n",
    "#attempt_counter_question = data.groupby('item_id').size().values\n",
    "attempt_counter_question = data.groupby('item_id').size().reset_index(name='count')\n",
    "\n",
    "# save the number of attempts per question as csv file\n",
    "attempt_counter_question = pd.DataFrame(attempt_counter_question)\n",
    "attempt_counter_question.to_csv(folder+'/'+ education_year+\"/processed/attempt_counter_question.csv\", index=False)\n",
    "\n",
    "# remove data to save memory\n",
    "del data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#228B22\">Step-2: Encoding Sparse Matrix</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build sparse features dataset from dense dataset and q-matrix\n",
    "\n",
    "<small>\n",
    "\n",
    "Arguments:\n",
    "- `df`: Dense dataset, output from the preprocessing function\n",
    "- `Q_mat`: Q-matrix, output from the preprocessing function\n",
    "- `active_features`: Features used to build the dataset (list of strings). Determined in the encoding parameters section.\n",
    "\n",
    "Output:\n",
    "- `sparse_df`: Sparse dataset. \n",
    "\n",
    "Depending on the encoding parameters, the sparse dataset is the one-hot encoded version of the dense dataset\n",
    "\n",
    "Example encoding that include the user-skill specialty_difficulty and item \n",
    "\n",
    "</small>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_sparse(df, Q_mat, active_features, config, skip_sucessive=True):\n",
    "    \n",
    "\t# Transform q-matrix into dictionary\n",
    "\tdt = time.time()\n",
    "\tdict_q_mat = {i:set() for i in range(Q_mat.shape[0])}\n",
    "\tfor elt in np.argwhere(Q_mat == 1):\n",
    "\t\tdict_q_mat[elt[0]].add(elt[1])\n",
    "\n",
    "\tX={}\n",
    "\tif 'skills' in active_features:\n",
    "\t\tX[\"skills\"] = sparse.csr_matrix(np.empty((0, Q_mat.shape[1])))\n",
    "\n",
    "\tX['df'] = np.empty((0,5)) # Keep only track of line index + user/item id + correctness\n",
    "\n",
    "\tq = defaultdict(lambda: OurQueue())  # Prepare counters for time windows\n",
    "\twf_counters = defaultdict(lambda: 0)\n",
    "\tif len(set(active_features).intersection({\"skills\",\"attempts\",\"wins\",\"fails\"})) > 0:\n",
    "\t\tfor stud_id in tqdm(df[\"user_id\"].unique()):\n",
    "\t\t\tdf_stud = df[df[\"user_id\"]==stud_id][[\"user_id\", \"item_id\", \"timestamp\",\"correct\",\"group\"]].copy()\n",
    "\t\t\tdf_stud_indices = np.array(df_stud.index).reshape(-1,1)\n",
    "\t\t\t#df_stud.sort_values(by=\"timestamp\", inplace=True) # Sort values\n",
    "\t\t\tdf_stud = np.array(df_stud)\n",
    "\t\t\tX['df'] = np.vstack((X['df'], np.hstack((df_stud[:,[0,1,3,4]], df_stud_indices))))\n",
    "\n",
    "\t\t\tskills_temp = Q_mat[df_stud[:,1].astype(int)].copy()\n",
    "\t\t\tif 'skills' in active_features:\n",
    "\t\t\t\tX['skills'] = sparse.vstack([X[\"skills\"],sparse.csr_matrix(skills_temp)])\n",
    "\t\t\t\t# if options.weighted_encoding == True:\n",
    "\t\t\t\t# \tX[\"skills\"]= X[\"skills\"]/X[\"skills\"].sum(axis=1)\n",
    "\t\t\t\n",
    "\tif 'users' in active_features:\n",
    "\t\tonehot = OneHotEncoder(categories=[np.arange(config[\"n_users\"])])\n",
    "\t\tif len(set(active_features).intersection({\"skills\",\"attempts\",\"wins\",\"fails\"})) > 0:\n",
    "\t\t\tX['users'] = onehot.fit_transform(X[\"df\"][:,0].reshape(-1,1))\n",
    "\t\telse:\n",
    "\t\t\tX['users'] = onehot.fit_transform(df[\"user_id\"].values.reshape(-1,1))\n",
    "\tif 'items' in active_features:\n",
    "\t\tonehot = OneHotEncoder(categories=[np.arange(config[\"n_items\"])])\n",
    "\t\tif len(set(active_features).intersection({\"skills\",\"attempts\",\"wins\",\"fails\"})) > 0:\n",
    "\t\t\tX['items'] = onehot.fit_transform(X[\"df\"][:,1].reshape(-1,1))\n",
    "\t\telse:\n",
    "\t\t\tX['items'] = onehot.fit_transform(df[\"item_id\"].values.reshape(-1,1))\n",
    "\n",
    "\tif len(set(active_features).intersection({\"skills\",\"attempts\",\"wins\",\"fails\"})) > 0:\n",
    "\t\tsparse_df = sparse.hstack([sparse.csr_matrix(X['df'][:,-3].reshape(-1,1)),\n",
    "\t\t\tsparse.hstack([X[agent] for agent in active_features]),sparse.csr_matrix(X['df'][:,-2].reshape(-1,1))]).tocsr()\n",
    "\t\tsparse_df = sparse_df[np.argsort(X[\"df\"][:,-1])] # sort matrix by original index\n",
    "\telse:\n",
    "\t\tsparse_df = sparse.hstack([sparse.csr_matrix(df[\"correct\"].values.reshape(-1,1)),\n",
    "\t\t\tsparse.hstack([X[agent] for agent in active_features]),sparse.csr_matrix(X['df'][:,-2].reshape(-1,1))]).tocsr()\n",
    "\t\t# No need to sort sparse matrix here\n",
    "\n",
    "\t# Split into train and test sparse matrices\n",
    "\ttrain_indices = np.nonzero(sparse_df[:, -1] == 0)[0]\n",
    "\ttest_indices = np.nonzero(sparse_df[:, -1] == 1)[0]\n",
    "\t# Extract train_sparse_df and test_sparse_df directly from the sparse matrix and remve the last column (group)\n",
    "\ttrain_sparse_df = sparse_df[train_indices, :-1]\n",
    "\ttest_sparse_df = sparse_df[test_indices, :-1]\n",
    "\n",
    "\treturn train_sparse_df, test_sparse_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(main_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data: 32422091 samples in  33.51363658905029 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24757/24757 [2:50:53<00:00,  2.41it/s]  \n",
      "C:\\Users\\Ghislaine\\AppData\\Local\\Temp\\ipykernel_12400\\2278624582.py:51: SparseEfficiencyWarning: Comparing a sparse matrix with 0 using == is inefficient, try using != instead.\n",
      "  train_sparse_df, test_sparse_df = df_to_sparse(df, qmat, active_features, dico)\n"
     ]
    }
   ],
   "source": [
    "if user_skill_item_already_encoded==True:\n",
    "\t# call X-uis.npz sparse matrixos.chdir(folder+'/'+ education_year+\"/processed\") \n",
    "\t# call X-uis.npz sparse matrix by having uis as features_suffix\n",
    "\tall_features = ['users', 'items', 'skills']\n",
    "\tactive_features = [features for features in all_features if vars(options)[features]]\n",
    "\tfeatures_suffix = ''.join([features[0] for features in active_features])\n",
    "\t# load the sparse matrix from folder+'/'+ education_year+\"/processed\"\n",
    "\t# go to folder+'/'+ education_year+\"/processed\"\n",
    "\tos.chdir(folder+'/'+ education_year+\"/processed\")\n",
    "\ttrain_sparse_df = sparse.load_npz('train_sparse_df-{:s}.npz'.format(features_suffix))\n",
    "\ttest_sparse_df = sparse.load_npz('test_sparse_df-{:s}.npz'.format(features_suffix))\n",
    "\n",
    "\twith open(\"config.json\") as f_in:\n",
    "\t\tdico = json.load(f_in)\n",
    "\n",
    "else:\n",
    "\tdt = time.time()\n",
    "\tos.chdir(folder+'/'+ education_year+'/processed')\n",
    "\tall_features = ['users', 'items', 'skills']\n",
    "\t#options = Options()\n",
    "\tactive_features = [features for features in all_features if vars(options)[features]]\n",
    "\tfeatures_suffix = ''.join([features[0] for features in active_features])\n",
    "\n",
    "\tif options.by_spec == True:\n",
    "\t\t# run for each specialty\n",
    "\t\tqmat = sparse.load_npz('q_mat.npz').toarray()\n",
    "\t\twith open(\"config.json\") as f_in:\n",
    "\t\t\tdico = json.load(f_in)\n",
    "\t\t# find the list of files that contain by_spec_preprocessed in their name\t\n",
    "\t\tfiles = [file for file in os.listdir() if 'by_spec_preprocessed' in file]\n",
    "\t\t# for file that contains by_spec_preprocessed in its name\n",
    "\t\tfor file in files:\n",
    "\t\t\tdf = pd.read_csv(file)\n",
    "\t\t\tprint('Loading data:', df.shape[0], 'samples in ', time.time() - dt, \"seconds\")\n",
    "\t\t\tX  = df_to_sparse(df, qmat, active_features, dico)\n",
    "\t\t\t# save the sparse matrix by specialty name and features_suffix\n",
    "\t\t\tname = file.split('by_spec_preprocessed_')[1].split('.csv')[0]\n",
    "\t\t\tsparse.save_npz('X-{:s}-{:s}.npz'.format(features_suffix, name), X)\n",
    "\n",
    "\telif options.by_spec == False:\n",
    "\t\tif options.file_splits == 1:\n",
    "\t\t\tdf = pd.read_csv('preprocessed_data.csv')\n",
    "\t\t\t# only keep the columns that are needed for the logistic reression model\n",
    "\t\t\tdf = df[['user_id', 'item_id', 'timestamp', 'correct', 'group']]\n",
    "\t\t\t# sort the data by user_id and timestamp\n",
    "\t\t\tdf = df.sort_values(by=[\"user_id\", \"timestamp\"]).reset_index(drop=True)\n",
    "\t\t\tqmat = sparse.load_npz('q_mat.npz').toarray()\n",
    "\t\t\twith open(\"config.json\") as f_in:\n",
    "\t\t\t\tdico = json.load(f_in)\n",
    "\t\t\tprint('Loading data:', df.shape[0], 'samples in ', time.time() - dt, \"seconds\")\n",
    "\t\t\ttrain_sparse_df, test_sparse_df = df_to_sparse(df, qmat, active_features, dico)\n",
    "\t\t\tsparse.save_npz('train_sparse_df-{:s}.npz'.format(features_suffix), train_sparse_df)\n",
    "\t\t\tsparse.save_npz('test_sparse_df-{:s}.npz'.format(features_suffix), test_sparse_df)\n",
    "\t\t\t# sparse.save_npz('train_sparse_df.npz'.format(features_suffix), train_sparse_df)\n",
    "\t\t\t# sparse.save_npz('test_sparse_df.npz'.format(features_suffix), test_sparse_df)\n",
    "\t\telif options.file_splits > 1:\n",
    "\t\t\tdf = pd.read_csv('preprocessed_data.csv')\n",
    "\t\t\t# only keep the columns that are needed for the logistic reression model\n",
    "\t\t\tdf = df[['user_id', 'item_id', 'timestamp', 'correct', 'group']]\n",
    "\t\t\t# sort the data by user_id and timestamp\n",
    "\t\t\tdf = df.sort_values(by=[\"user_id\", \"timestamp\"]).reset_index(drop=True)\n",
    "\t\t\tqmat = sparse.load_npz('q_mat.npz').toarray()\n",
    "\t\t\twith open(\"config.json\") as f_in:\n",
    "\t\t\t\tdico = json.load(f_in)\n",
    "\t\t\tprint('Loading data:', df.shape[0], 'samples in ', time.time() - dt, \"seconds\")\n",
    "\t\t\tlist_of_user_ids = np.array_split(np.arange(dico[\"n_users\"]),options.file_splits)\n",
    "\t\t\t# remove old sparse dataframes in the folder\n",
    "\t\t\tfor file in glob.glob(\"train_sparse_df-{:s}_[0-9].npz\".format(features_suffix)):\n",
    "\t\t\t\tos.remove(file)\n",
    "\t\t\tfor file in glob.glob(\"test_sparse_df-{:s}_[0-9].npz\".format(features_suffix)):\n",
    "\t\t\t\tos.remove(file)\n",
    "\t\t\t# for each split, save the train and test sparse dataframes\n",
    "\t\t\tfor i, arr in enumerate(list_of_user_ids):\n",
    "\t\t\t\tdf = pd.read_csv('preprocessed_data.csv')\n",
    "\t\t\t\t# only keep the columns that are needed for the logistic reression model\n",
    "\t\t\t\tdf = df[['user_id', 'item_id', 'timestamp', 'correct', 'group']]\n",
    "\t\t\t\t# sort the data by user_id and timestamp\n",
    "\t\t\t\tdf.sort_values(by=[\"user_id\",\"timestamp\"], inplace=True)\n",
    "\t\t\t\tdf = df[df[\"user_id\"].isin(arr)]\n",
    "\t\t\t\ttrain_sparse_df, test_sparse_df  = df_to_sparse(df, qmat, active_features, dico)\n",
    "\t\t\t\tsparse.save_npz('train_sparse_df-{:s}_{}.npz'.format(features_suffix,i), train_sparse_df)\n",
    "\t\t\t\tsparse.save_npz('test_sparse_df-{:s}_{}.npz'.format(features_suffix,i), test_sparse_df)\n",
    "\t\t\t\t# sparse.save_npz('train_sparse_df.npz'.format(features_suffix), train_sparse_df)\n",
    "\t\t\t\t# sparse.save_npz('test_sparse_df.npz'.format(features_suffix), test_sparse_df)\n",
    "\t\t\t# TODO : remove old sparse dataframes\n",
    "\t\t\ttrain_sparse_df = sparse.vstack([sparse.load_npz(sparse_file) for sparse_file in sorted(glob.glob(\"train_sparse_df-{:s}_[0-9].npz\".format(features_suffix)))])\n",
    "\t\t\tsparse.save_npz('train_sparse_df-{:s}.npz'.format(features_suffix), train_sparse_df)\n",
    "\t\t\ttest_sparse_df= sparse.vstack([sparse.load_npz(sparse_file) for sparse_file in sorted(glob.glob(\"test_sparse_df-{:s}_[0-9].npz\".format(features_suffix)))])\n",
    "\t\t\tsparse.save_npz('test_sparse_df-{:s}.npz'.format(features_suffix), test_sparse_df)\n",
    "\t\telse:\n",
    "\t\t\tprint(\"Please select file_splits >= 1.\")\n",
    "\telse:\n",
    "\t\tprint(\"Please select by_spec = True or False.\")\n",
    "\n",
    "\t# call X-uis.npz sparse matrix\n",
    "\t#os.chdir(folder+'/'+ education_year+\"/processed\")\n",
    "\t# call X-uis.npz sparse matrix by having uis as features_suffix\n",
    "\tall_features = ['users', 'items', 'skills']\n",
    "\tactive_features = [features for features in all_features if vars(options)[features]]\n",
    "\tfeatures_suffix = ''.join([features[0] for features in active_features])\n",
    "\ttrain_sparse_df = sparse.load_npz('train_sparse_df-{:s}.npz'.format(features_suffix))\n",
    "\ttest_sparse_df = sparse.load_npz('test_sparse_df-{:s}.npz'.format(features_suffix))\n",
    "\n",
    "\twith open(\"config.json\") as f_in:\n",
    "\t\tdico = json.load(f_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(main_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if options.user_skill_together == True:\n",
    "    \n",
    "    # both for train and test sparse seperately\n",
    "    for X in [train_sparse_df, test_sparse_df]:\n",
    "\n",
    "        # find the columns 1:n_users of X\n",
    "        users_sparse = X[:, 1:dico[\"n_users\"]+1]\n",
    "        indices_user = users_sparse.nonzero()\n",
    "        nonzero_indices_user = list(zip(indices_user[0], indices_user[1]))\n",
    "\n",
    "        # find the last n_skills columns of X\n",
    "        skills_sparse = X[:, -dico[\"n_skills\"]:]\n",
    "        indices_skills = skills_sparse.nonzero()\n",
    "        nonzero_indices_skills = list(zip(indices_skills[0], indices_skills[1]))\n",
    "\n",
    "        # find the columns in between users_sparse and skills_sparse\n",
    "        questions_sparse = X[:, dico[\"n_users\"]+1:-dico[\"n_skills\"]]\n",
    "\n",
    "        # find the first column of X\n",
    "        correct_sparse = X[:, 0]\n",
    "\n",
    "        # Get the number of columns in the sparse matrices\n",
    "        num_users = users_sparse.shape[1]\n",
    "        num_skills = skills_sparse.shape[1]\n",
    "        # Create the COO matrix\n",
    "        row_indices = [row for row, _ in nonzero_indices_skills]\n",
    "        col_indices = [nonzero_indices_user[row][1] * num_skills + col for row, col in nonzero_indices_skills]\n",
    "        user_skill_coo = sparse.coo_matrix((np.ones(len(row_indices)), (row_indices, col_indices)),\n",
    "                                            shape=(users_sparse.shape[0], num_users * num_skills),\n",
    "                                            dtype=np.float64)\n",
    "\n",
    "        user_skill_sparse = user_skill_coo.tocsr()\n",
    "\n",
    "        if options.weighted_encoding == True:\n",
    "            # user_skill_sparse= user_skill_sparse/user_skill_sparse.sum(axis=1)\n",
    "            # skills_sparse= skills_sparse/skills_sparse.sum(axis=1)\n",
    "            user_skill_sparse= normalize(user_skill_sparse, norm='l1')\n",
    "            skills_sparse= normalize(skills_sparse, norm='l1')\n",
    "            \n",
    "        if options.spec_difficulty==True:\n",
    "            X_sparse = sparse.hstack([correct_sparse, user_skill_sparse, skills_sparse, questions_sparse])\n",
    "        else:\n",
    "            X_sparse = sparse.hstack([correct_sparse, user_skill_sparse, questions_sparse])\n",
    "            \n",
    "        # if X  is in the first iteration (train_sparse), then save it as X-uuis.npz\n",
    "        # if first iteration\n",
    "        if X is train_sparse_df:\n",
    "            #  save X_sparse as train_sparse_ready.npz to EXPERIMENT_FOLDER\n",
    "            sparse.save_npz(os.path.join(EXPERIMENT_FOLDER,'train_sparse_ready.npz'), X_sparse)\n",
    "        # if second iteration\n",
    "        if X is test_sparse_df:\n",
    "            # save X_sparse as test_sparse_ready.npz to EXPERIMENT_FOLDER\n",
    "            sparse.save_npz(os.path.join(EXPERIMENT_FOLDER,'test_sparse_ready.npz'), X_sparse)\n",
    "            \n",
    "        \n",
    "            \n",
    "else:\n",
    "    # save train_sparse\n",
    "    sparse.save_npz(os.path.join(EXPERIMENT_FOLDER,'train_sparse_ready.npz'), train_sparse_df)\n",
    "    # save test_sparse\n",
    "    sparse.save_npz(os.path.join(EXPERIMENT_FOLDER,'test_sparse_ready.npz'), test_sparse_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#228B22\">Step-3: Run Logistic Regression</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(main_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting...\n",
      "Expected number of coefs: 865145. Real number of coefs: 865145\n"
     ]
    }
   ],
   "source": [
    "experiment_args = vars(options)\n",
    "today = datetime.datetime.now() # save date of experiment\n",
    "\n",
    "# EXPERIMENT_FOLDER = folder + '/' + education_year + '/results/'\n",
    "#dataio.prepare_folder(EXPERIMENT_FOLDER)\n",
    "# load config file\n",
    "with open(f'{folder}{education_year}/processed/config.json') as json_file:\n",
    "    config = json.load(json_file)\n",
    "    n_items = config[\"n_items\"]\n",
    "    n_users = config[\"n_users\"]\n",
    "    n_skills= config[\"n_skills\"]\n",
    "    \n",
    "if options.by_spec == True:\n",
    "    learner_competency = np.zeros((uSize, tSize)) \n",
    "    attempt_counter_student_spec=np.zeros((uSize, tSize))\n",
    "\n",
    "    for specialty in skill_names_ids_map_df[\"specialty\"].unique():\n",
    "        \n",
    "        X=csr_matrix(load_npz(f\"{folder}{education_year}/processed/X-{features_suffix}-{specialty}.npz\"))\n",
    "        y = X[:,0].toarray().flatten()\n",
    "        dt = time.time()\n",
    "        # print fittin..message by also mentioning the specialty\n",
    "        print(\"Fitting logistic regression for specialty: {}...\".format(specialty))\n",
    "        lr = LogisticRegression(solver=\"saga\", max_iter=options.iter, C=options.C,n_jobs=-1).fit(X[:,1:],y)\n",
    "        \n",
    "        # find the corresponding specialty_id from skill_names_ids_map_df\n",
    "        specialty_id = skill_names_ids_map_df[skill_names_ids_map_df[\"specialty\"]==specialty][\"specialty_id\"].values[0]\n",
    "        user_deltas = lr.coef_[:,:n_users]\n",
    "        # fill the learner_competency matrix with the corresponding specialty_id\n",
    "        learner_competency[:,specialty_id] = user_deltas.flatten()\n",
    "        \n",
    "        # load data for this specialty\n",
    "        data_this_spec= pd.read_csv(f\"{folder}{education_year}/processed/by_spec_preprocessed_{specialty}.csv\")\n",
    "        # count number of attempts per student\n",
    "        num_attempts_per_student = data_this_spec.groupby(\"user_id\").size()\n",
    "        # fill the attempt_counter_student_spec matrix with the corresponding specialty_id and for the students who attempted the specialty\n",
    "        attempt_counter_student_spec[num_attempts_per_student.index,specialty_id] = num_attempts_per_student.values\n",
    "\n",
    "    # rename the columns of the learner_competency matrix with the specialty names\n",
    "    learner_competency= pd.DataFrame(learner_competency, columns=skill_names_ids_map_df[\"specialty\"].unique())\n",
    "    attempt_counter_student_spec= pd.DataFrame(attempt_counter_student_spec, columns=skill_names_ids_map_df[\"specialty\"].unique())\n",
    "    # save the learner_competency as csv\n",
    "    learner_competency.index.name = \"user_id\"\n",
    "    learner_competency.to_csv(os.path.join(EXPERIMENT_FOLDER,'learner_competency.csv'))\n",
    "    # save the attempt_counter_student_spec as csv to processed folder and rename indices as usedr_id\n",
    "    attempt_counter_student_spec.index.name = \"user_id\"\n",
    "    attempt_counter_student_spec.to_csv(folder+'/'+ education_year+\"/processed/attempt_counter_student_spec.csv\")\n",
    "    \n",
    "    \n",
    "else:\n",
    "    \n",
    "    # Load sparsely encoded datasets from EXPERIMENT_FOLDER\n",
    "    #X = csr_matrix(load_npz(f\"{folder}{education_year}/processed/train_sparse_ready.npz\"))\n",
    "    X = csr_matrix(load_npz(os.path.join(EXPERIMENT_FOLDER, \"train_sparse_ready.npz\")))\n",
    "\n",
    "    #X = csr_matrix(load_npz(f\"{folder}{education_year}/processed/X-{features_suffix}.npz\"))\n",
    "    #X = csr_matrix(load_npz(folder+'/'+education_year+'/processed/X-uuis.npz'))\n",
    "    y = X[:,0].toarray().flatten()\n",
    "    #qmat = load_npz(f\"{folder}{education_year}/processed/q_mat.npz\")\n",
    "    #n_items = qmat.shape[0]\n",
    "\n",
    "    dt = time.time()\n",
    "    print('fitting...')\n",
    "\n",
    "    lr = LogisticRegression(solver=\"saga\", max_iter=options.iter, C=options.C,n_jobs=-1).fit(X[:,1:],y)\n",
    "    # save the model\n",
    "    joblib.dump(lr, EXPERIMENT_FOLDER+'logistic_regression_model.pkl')\n",
    "    # print the expected number of coefs and the real number of coefs\n",
    "    if options.spec_difficulty == True:\n",
    "        expected_number_of_coefs = n_users*n_skills+n_skills+n_items\n",
    "    elif options.spec_difficulty == False:\n",
    "        expected_number_of_coefs = n_users*n_skills+n_items\n",
    "    print(\"Expected number of coefs: {}. Real number of coefs: {}\".format(expected_number_of_coefs,len(lr.coef_[0])))\n",
    "    \n",
    "    # saving the output of the logistic regression to the appropriate folder\n",
    "    \n",
    "    if options.user_skill_together == True:\n",
    "        \n",
    "        # item_difficulty\n",
    "        \n",
    "        # Load item_deltas array from saved numpy file\n",
    "        item_deltas = np.array(lr.coef_[0][-n_items:])\n",
    "        item_indices = np.arange(item_deltas.shape[0])\n",
    "        # item_indices as integers\n",
    "        item_indices = item_indices.astype(int)\n",
    "        # Combine item indices and deltas into a single array\n",
    "        item_difficulty = np.column_stack((item_indices, item_deltas))\n",
    "        # Convert to pandas DataFrame and rename columns\n",
    "        item_difficulty_df = pd.DataFrame(item_difficulty, columns=['item_id', 'difficulty_irt'])\n",
    "        # Save as CSV file\n",
    "        csv_path = os.path.join(EXPERIMENT_FOLDER, 'question_difficulty.csv')\n",
    "        item_difficulty_df.to_csv(csv_path, index=False)\n",
    "        \n",
    "        # learner_competency\n",
    "        \n",
    "        skill_names_ids_map_df = pd.read_csv(folder+'/'+education_year+'/processed/skill_names_ids_map.csv')\n",
    "        learner_competency=np.array(lr.coef_[0][:n_users*n_skills]).reshape(n_users,n_skills)\n",
    "        # turn into pandas dataframe\n",
    "        learner_competency= pd.DataFrame(learner_competency, columns=skill_names_ids_map_df[\"specialty\"].unique())\n",
    "        # index as user_id\n",
    "        learner_competency.index.name = \"user_id\"\n",
    "        # save as csv\n",
    "        csv_path = os.path.join(EXPERIMENT_FOLDER, 'learner_competency.csv')\n",
    "        learner_competency.to_csv(csv_path)\n",
    "        \n",
    "        # specialty diffciulty\n",
    "        \n",
    "        if options.spec_difficulty==True:\n",
    "            # open an empty specialty_difficulty dataframe with specialty and specialty_difficulty as columns\n",
    "            specialty_difficulty = pd.DataFrame(columns=[\"specialty\",\"specialty_difficulty\"])\n",
    "            # fill the specialty column with the skill_names_ids_map_df[\"specialty\"].unique()\n",
    "            specialty_difficulty[\"specialty\"] = skill_names_ids_map_df[\"specialty\"].unique()\n",
    "            # fill the specialty_difficulty column with the lr.coef_[0][n_users*n_skills:(n_users*n_skills+n_skills)]\n",
    "            specialty_difficulty[\"specialty_difficulty\"] = lr.coef_[0][n_users*n_skills:(n_users*n_skills+n_skills)]\n",
    "            # remove index column\n",
    "            specialty_difficulty.index.name = None\n",
    "            # save as csv\n",
    "            csv_path = os.path.join(EXPERIMENT_FOLDER, 'specialty_difficulty.csv')\n",
    "            specialty_difficulty.to_csv(csv_path)\n",
    "\n",
    "                \n",
    "        #  attempts per student in each specialty\n",
    "        \n",
    "        # Filter the 1:n_students x n_skills columns\n",
    "        X_filtered = X[:, 1:n_users * n_skills + 1]\n",
    "        # Sum the columns to find the number of attempts per student\n",
    "        if options.weighted_encoding == True:\n",
    "            X_filtered[X_filtered != 0] = 1\n",
    "        num_attempts_per_student = np.asarray(X_filtered.sum(axis=0)).ravel()\n",
    "        # Reshape the attempts array to match n_users x n_skills\n",
    "        attempt_counter_student = num_attempts_per_student.reshape(n_users, n_skills)\n",
    "        # Create a DataFrame with the attempt_counter_student array\n",
    "        attempt_counter_student_df = pd.DataFrame(attempt_counter_student, columns=skill_names_ids_map_df[\"specialty\"].unique())\n",
    "        # Set the index name to \"user_id\"\n",
    "        attempt_counter_student_df.index.name = \"user_id\"\n",
    "        # Save the DataFrame to CSV\n",
    "        attempt_counter_student_df.to_csv(folder + '/' + education_year + '/processed/attempt_counter_student_spec.csv')\n",
    "        \n",
    "        # Attempt per specialty\n",
    "\n",
    "        # Get the sum of the rows in the attempt_counter_student DataFrame\n",
    "        attempt_counter_spec = attempt_counter_student_df.sum(axis=0)\n",
    "        # Create a new DataFrame with the total_attempts column\n",
    "        attempt_counter_spec_df = attempt_counter_spec.to_frame()\n",
    "        # Rename the column as \"total_attempts\"\n",
    "        attempt_counter_spec_df.columns = [\"total_attempts\"]\n",
    "        # Rename the index as \"specialty\"\n",
    "        attempt_counter_spec_df.index.name = \"specialty\"\n",
    "        # Save the DataFrame to CSV\n",
    "        attempt_counter_spec_df.to_csv(folder + '/' + education_year + '/processed/attempt_counter_spec.csv')    \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        \n",
    "        #np.save(os.path.join(EXPERIMENT_FOLDER,'item_deltas.npy'), np.array(lr.coef_[0,-n_items:]))\n",
    "        #np.save(os.path.join(EXPERIMENT_FOLDER,'item_deltas.npy'), np.array(lr.coef_[0][-n_items:]))\n",
    "        np.save(os.path.join(EXPERIMENT_FOLDER,'item_deltas.npy'), np.array(lr.coef_[0][-(n_skills+n_items):-n_skills]))\n",
    "        # save the last n_skills coefs as skill_deltas\n",
    "        np.save(os.path.join(EXPERIMENT_FOLDER,'skill_deltas.npy'), np.array(lr.coef_[0][-n_skills:]))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalculateRMSE(Output, ground, I):\n",
    "    Output = np.array(Output)\n",
    "    ground = np.array(ground)\n",
    "    error = (Output - ground) \n",
    "    err_sqr = error*error\n",
    "    RMSE = math.sqrt(err_sqr.sum()/I)\n",
    "    return RMSE  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE:  0.4349058422885915\n",
      "Test AUC:  0.7840478106690244\n",
      "Test ACC:  0.7078667053508517\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "import math\n",
    "\n",
    "# validate the test set by using the trained model\n",
    "# load the test_sparse_df\n",
    "#test_sparse_df = sparse.load_npz(folder+'/'+education_year+'/processed/test_sparse_ready.npz')\n",
    "test_sparse_df = sparse.load_npz(os.path.join(EXPERIMENT_FOLDER, \"train_sparse_ready.npz\"))\n",
    "X = csr_matrix(load_npz(os.path.join(EXPERIMENT_FOLDER, \"train_sparse_ready.npz\")))\n",
    "\n",
    "# read lr model from the EXPERIMENT_FOLDER\n",
    "lr = joblib.load(os.path.join(EXPERIMENT_FOLDER, \"logistic_regression_model.pkl\"))\n",
    "# predict the probability of correctness for each sample in the test_sparse_df\n",
    "y_pred = lr.predict_proba(test_sparse_df[:,1:])\n",
    "# save the predicted probabilities as csv\n",
    "#np.savetxt(folder+'/'+education_year+'/results/y_pred.csv', y_pred, delimiter=',')\n",
    "np.savetxt(os.path.join(EXPERIMENT_FOLDER, \"train_sparse_ready.npz\"), y_pred, delimiter=',')\n",
    "# compare the predicted probabilities with the actual correctness\n",
    "# load the actual correctness\n",
    "test_target = test_sparse_df[:,0].toarray().flatten()\n",
    "# compute the roc_auc_score\n",
    "auc_score=roc_auc_score(test_target, y_pred[:,1])\n",
    "\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_predictions = lr.predict(test_sparse_df[:, 1:]) \n",
    "accuracy = accuracy_score(test_target, test_predictions)\n",
    "precision = precision_score(test_target, test_predictions)\n",
    "recall = recall_score(test_target, test_predictions)\n",
    "f1 = f1_score(test_target, test_predictions)\n",
    "\n",
    "\n",
    "rmse_test = CalculateRMSE( y_pred[:,1], test_target, len(y_pred[:,1]))\n",
    "\n",
    "print(\"Test RMSE: \", rmse_test)\n",
    "print(\"Test AUC: \", auc_score)\n",
    "print(\"Test ACC: \", accuracy)\n",
    "\n",
    "\n",
    "# save the results to EXPERIMENT_FOLDER\n",
    "with open(os.path.join(EXPERIMENT_FOLDER, \"validation_results.txt\"), \"w\") as f:\n",
    "    f.write(\"Test RMSE: \" + str(rmse_test) + \"\\n\")\n",
    "    f.write(\"Test AUC: \" + str(auc_score) + \"\\n\")\n",
    "    f.write(\"Test ACC: \" + str(accuracy) + \"\\n\")\n",
    "    f.write(\"Test Precision: \" + str(precision) + \"\\n\")\n",
    "    f.write(\"Test Recall: \" + str(recall) + \"\\n\")\n",
    "    f.write(\"Test F1: \" + str(f1) + \"\\n\")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
