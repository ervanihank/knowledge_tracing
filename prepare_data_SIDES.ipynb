{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "def prvar(__x):\n",
    "    print(traceback.extract_stack(limit=2)[0][3][6:][:-1],\"=\",__x)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For TEST\n",
    "\n",
    "# folder_name= 'data/kdd'\n",
    "# course_name= 'sides_20_21'\n",
    "# train_file= 'SIDES_20_21_6thyear_withspec_training_prepared.csv'\n",
    "# test_file= 'SIDES_20_21_6thyear_withspec_test_prepared.csv'\n",
    "# kc_col_name= 'specialty'\n",
    "# min_interactions_per_user= 100\n",
    "# remove_nan_skills= True\n",
    "# verbose= True\n",
    "# drop_duplicates= True\n",
    "# remove_nan_answer_type= True\n",
    "# min_answer_per_question= 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sides(folder_name, course_name, train_file, test_file, kc_col_name, min_interactions_per_user, min_answer_per_question, remove_nan_skills, remove_nan_answer_type, verbose, drop_duplicates=True):\n",
    "    '''\n",
    "    Reading input files\n",
    "    Drop rows for which topic is not determined\n",
    "    Return the pre-processed file and Q-matrix (for question-specialty matches).\n",
    "    \n",
    "    Arguments:\n",
    "    folder_name -- path to the folder containig kdd files (algebra05, bridge_algebra06)\n",
    "    course_name -- name of the course for which pre_processing is executed\n",
    "    train_file -- original train_file provided by KDD cup organizers\n",
    "    test_file -- original test_file provided by KDD cup organizers\n",
    "    kc_col_name -- Skills id column\n",
    "    min_interactions_per_user -- minimum number of interactions per student\n",
    "    drop_duplicates -- if True, drop duplicates from dataset\n",
    "    \n",
    "    Outputs:\n",
    "    data -- preprocessed dataset (pandas DataFrame)\n",
    "    Q_mat -- corresponding q-matrix (item-skill relationships sparse array)\n",
    "    '''\n",
    "    \n",
    "    if not os.path.exists(folder_name):\n",
    "        print(\"The provided path for the data is invalid and the function will not be executed.\")\n",
    "        return None, None, None, None\n",
    "        \n",
    "    # reading csv file containing information about students' practice (attempt) history\n",
    "    # from the train and test file provided by KDD organizer and then concatante them.\n",
    "    train_file_path = folder_name  +'/'+  course_name  +'/'+ train_file\n",
    "    df_train = pd.read_csv(train_file_path).rename(columns={\n",
    "        'student': 'student',\n",
    "        'question': 'question',\n",
    "        kc_col_name: 'kc_id',\n",
    "        'date_time': 'timestamp',\n",
    "        'result': 'correct',\n",
    "        'n_answer_option': 'n_options',\n",
    "        'answer_option': 'answer_type',\n",
    "        'type': 'quest_type',\n",
    "    })[['student', 'question','correct', 'timestamp', 'kc_id','n_options','answer_type','quest_type']]\n",
    "    \n",
    "    if verbose:\n",
    "        initial_shape = df_train.shape[0]\n",
    "        print(\"Opened SIDES train data. Output: {} samples.\".format(initial_shape))\n",
    "        \n",
    "    test_file_path = folder_name  +'/'+  course_name  +'/'+ test_file\n",
    "    df_test = pd.read_csv(test_file_path).rename(columns={\n",
    "        'student': 'student',\n",
    "        'question': 'question',\n",
    "        kc_col_name: 'kc_id',\n",
    "        'date_time': 'timestamp',\n",
    "        'result': 'correct',\n",
    "        'n_answer_option': 'n_options',\n",
    "        'answer_option': 'answer_type',\n",
    "        'type': 'quest_type',\n",
    "    })[['student', 'question', 'correct', 'timestamp', 'kc_id','n_options','answer_type','quest_type']]\n",
    "    if verbose:\n",
    "        initial_shape = df_test.shape[0]\n",
    "        print(\"Opened SIDES test data. Output: {} samples.\".format(initial_shape))\n",
    "        \n",
    "    #merge df_tarin & df_test    \n",
    "    df_train['group'] = 0\n",
    "    df_test['group'] = 1\n",
    "    frames = [df_train, df_test]\n",
    "    data = pd.concat(frames)\n",
    "    del df_train\n",
    "    del df_test\n",
    "    \n",
    "    # Remove potential duplicates\n",
    "    #data= df_train.copy()\n",
    "    initial_shape = data.shape[0]\n",
    "    data = data[~data.duplicated()]\n",
    "    if verbose:\n",
    "        print(\"Removed {} duplicated samples.\".format(initial_shape-data.shape[0]))\n",
    "    initial_shape = data.shape[0]\n",
    "    \n",
    "    #removing rows with empty value for KC from our dataframe\n",
    "    if remove_nan_skills:\n",
    "        initial_shape=data.shape[0]\n",
    "        data = data[~data[\"kc_id\"].isnull()]\n",
    "        if verbose:\n",
    "            print(\"Removed {} samples with NaN skills.\".format(initial_shape-data.shape[0]))\n",
    "        initial_shape = data.shape[0]\n",
    "    else:\n",
    "        data.loc[data[\"kc_id\"].isnull(), \"kc_id\"] = 'NaN'\n",
    "        \n",
    "        \n",
    "    #removing rows with NAN value for n_correct_options from our dataframe\n",
    "    if remove_nan_answer_type:\n",
    "        initial_shape=data.shape[0]\n",
    "        data = data.dropna(subset=[\"answer_type\"])\n",
    "        if verbose:\n",
    "            print(\"Removed {} samples with NA answer_type.\".format(initial_shape-data.shape[0]))\n",
    "        initial_shape = data.shape[0]\n",
    "            \n",
    "    # Add correct time/date columns\n",
    "    original_format = \"%Y-%m-%dT%H:%M:%SZ\"\n",
    "    desired_format = \"%Y%m%d%H%M%S\"\n",
    "    data[\"timestamp\"] = pd.to_datetime(data[\"timestamp\"])\n",
    "    data[\"timestamp\"] = data[\"timestamp\"].apply(lambda x: x.strftime(desired_format))\n",
    "\n",
    "    ## instead of the one above, we use the following code to get the correct time (if the time is in the format of integer)\n",
    "    #data[\"correct_time\"] = data[\"time\"].apply(lambda x:str(x).zfill(6))\n",
    "    #data[\"correct_date\"] = data[\"date\"].apply(lambda x:str(x))\n",
    "    #full_dates = [str1+str2 for str1, str2 in zip(data[\"correct_date\"],data[\"correct_time\"])]\n",
    "    #data[\"full_time\"] = full_dates\n",
    "    #data = data[[\"user_id\",\"item_id\",\"full_time\",\"correct\",\"group\"]]\n",
    "\n",
    "    data[\"timestamp\"] = pd.to_datetime(data.loc[:,\"timestamp\"],format=\"%Y%m%d%H%M%S\")\n",
    "    data.loc[:,\"timestamp\"] = data.loc[:,\"timestamp\"] - data.loc[:,\"timestamp\"].min()\n",
    "    data.loc[:,\"timestamp\"] = data.loc[:,\"timestamp\"].apply(lambda x: x.total_seconds()).astype(np.int64)\n",
    "\n",
    "\n",
    "    # binarize correct column\n",
    "    data.correct = data.correct.apply(lambda x:0 if x not in [0,1] else x).astype(np.int32)\n",
    "    data = data[data['correct'].isin([0,1])] # Remove potential continuous outcomes\n",
    "    if verbose:\n",
    "        print(\"Removed {} samples with non-binary outcomes.\".format(initial_shape-data.shape[0]))\n",
    "    initial_shape = data.shape[0]\n",
    "    data['correct'] = data['correct'].astype(np.int32) # Cast outcome as int32\n",
    "    \n",
    "    \n",
    "    #removing users without enough interaction (min_interactions_per_user)\n",
    "    data = data.groupby(\"student\").filter(lambda x: len(x) >= min_interactions_per_user)\n",
    "    if verbose:\n",
    "        print('Removed {} samples '.format((initial_shape-data.shape[0])))\n",
    "        print('(users with less than {} interactions).'.format((min_interactions_per_user)))\n",
    "    initial_shape = data.shape[0]\n",
    "        \n",
    "    #removing questions without enough answers (min_answer_per_question)\n",
    "    data = data.groupby(\"question\").filter(lambda x: len(x) >= min_answer_per_question)\n",
    "    if verbose:\n",
    "        print('Removed {} samples '.format((initial_shape-data.shape[0])))\n",
    "        print('(questions with less than {} answers).'.format((min_answer_per_question)))\n",
    "    initial_shape = data.shape[0]\n",
    "        \n",
    "    # Create variables\n",
    "    #data[\"item_id\"] = data[\"pb_id\"]\n",
    "    data = data[['student', 'question', 'n_options','answer_type','kc_id', 'correct', 'timestamp', 'group']]\n",
    "        \n",
    "    # Transform ids into numeric\n",
    "    data[\"user_id\"] = np.unique(data[\"student\"], return_inverse=True)[1].astype(np.int64)\n",
    "    data[\"item_id\"] = np.unique(data[\"question\"], return_inverse=True)[1].astype(np.int64)\n",
    "    \n",
    "    # Rename questions/skills in item_skills data\n",
    "    old_new_item_ids = data[~data.duplicated([\"question\",\"item_id\"])][[\"question\",\"item_id\"]]\n",
    "    old_new_user_ids = data[~data.duplicated([\"student\",\"user_id\"])][[\"student\",\"user_id\"]]\n",
    "    if not os.path.isdir(folder_name+'/'+ course_name+\"/processed\"):\n",
    "        os.makedirs(folder_name+'/'+ course_name+\"/processed\")\n",
    "    old_new_item_ids.to_csv(folder_name+'/'+ course_name+\"/processed/old_new_item_ids.csv\",index=False)\n",
    "    old_new_user_ids.to_csv(folder_name+'/'+ course_name+\"/processed/old_new_user_ids.csv\",index=False)\n",
    "    \n",
    "    # # call skills data\n",
    "    # spec_file= folder_name  +'/'+  course_name  +'/'+ 'questions_specialty.csv'\n",
    "    # item_skills = pd.read_csv(spec_file)\n",
    "    # item_skills = item_skills.merge(old_new_item_ids,on=\"question\",how=\"right\")\n",
    "    \n",
    "    # keep only the necessary columns\n",
    "    data = data[['user_id', 'item_id', 'n_options','answer_type' ,'timestamp', 'correct', 'kc_id', 'group']]\n",
    "    \n",
    "    # To be safe, drop duplicates again\n",
    "    data.drop_duplicates(inplace=True)\n",
    "    if verbose:\n",
    "        if data.shape[0] < initial_shape:\n",
    "            logging.warning(\"{} duplicates have been found before saving the CSV.\".format(initial_shape-data.shape[0]))\n",
    "    initial_shape = data.shape[0]    \n",
    "    \n",
    "    # Create list of KCs\n",
    "    listOfKC = []\n",
    "    for kc_raw in data[\"kc_id\"].unique():\n",
    "        for elt in kc_raw.split('+'):\n",
    "            listOfKC.append(elt)\n",
    "    listOfKC = np.unique(listOfKC)\n",
    "\n",
    "    dict1_kc = {}\n",
    "    dict2_kc = {}\n",
    "    for k, v in enumerate(listOfKC):\n",
    "        dict1_kc[v] = k\n",
    "        dict2_kc[k] = v\n",
    "        \n",
    "        \n",
    "    # Build Q-matrix\n",
    "    Q_mat = np.zeros((len(data[\"item_id\"].unique()), len(listOfKC)))\n",
    "    item_skill = np.array(data[[\"item_id\",\"kc_id\"]])\n",
    "    for i in range(len(item_skill)):\n",
    "        splitted_kc = item_skill[i,1].split('+')\n",
    "        for kc in splitted_kc:\n",
    "            Q_mat[item_skill[i,0],dict1_kc[kc]] = 1\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Computed q-matrix. Shape: {}.\".format(Q_mat.shape))\n",
    "        \n",
    "    print(\"Data preprocessing done. Final output: {} samples.\".format((data.shape[0])))   \n",
    "\n",
    "\n",
    "    # Save preprocessed data\n",
    "    #data['timestamp'] =  pd.to_datetime(data['timestamp'])#, dayfirst=True)\n",
    "    data.sort_values(by=[\"timestamp\", \"item_id\"], inplace=True)#first, timestamp should be converted to datetime\n",
    "    data.reset_index(inplace=True, drop=True) \n",
    "    sparse.save_npz(folder_name+'/'+ course_name+\"/processed/q_mat.npz\", sparse.csr_matrix(Q_mat))\n",
    "    data.to_csv(folder_name+'/'+ course_name+\"/processed/preprocessed_data.csv\", index=False)\n",
    "    \n",
    "    # split train and test and save them\n",
    "    train_set = data[data['group'] == 0]\n",
    "    train_set.reset_index(inplace=True, drop=True)\n",
    "    #train_set['timestamp'] =  pd.to_datetime(train_set['timestamp'])#, dayfirst=True)\n",
    "    train_set.sort_values(by=[\"timestamp\", \"item_id\"], inplace=True) #first, timestamp should be converted to datetime\n",
    "    train_set.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    test_set = data[data['group'] == 1]\n",
    "    test_set.reset_index(inplace=True, drop=True)\n",
    "    #test_set['timestamp'] =  pd.to_datetime(test_set['timestamp'])#, dayfirst=True)\n",
    "    test_set.sort_values(by=[\"timestamp\", \"item_id\"], inplace=True)#first, timestamp should be converted to datetime\n",
    "    test_set.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    train_set.to_csv(folder_name+'/'+ course_name+\"/processed/train_set.csv\", encoding='utf-8', index = False)\n",
    "    test_set.to_csv(folder_name+'/'+ course_name+\"/processed/test_set.csv\", encoding='utf-8', index = False)\n",
    "    \n",
    "    # save skill_names_ids_map\n",
    "    # Convert the NumPy array to a pandas DataFrame and first column as specilaty column and  indices as  specilat_id column\n",
    "    skill_names_ids_map_df = pd.DataFrame(listOfKC,columns=['specialty'])\n",
    "    # have a specialty id column with values from 0 to n_skills\n",
    "    skill_names_ids_map_df['specialty_id'] = skill_names_ids_map_df.index\n",
    "    # Save the DataFrame as a CSV file\n",
    "    skill_names_ids_map_df.to_csv(folder_name+'/'+ course_name + '/processed/skill_names_ids_map.csv', index=False)\n",
    "\n",
    "    listOfKC = list(listOfKC)\n",
    "    # save listOfKC list\n",
    "    with open(folder_name+'/'+ course_name+'/processed/listOfKC.json', 'w') as fp:\n",
    "        json.dump(listOfKC, fp)\n",
    "    \n",
    "    # Save dict1_kc\n",
    "    with open(folder_name+'/'+ course_name+'/processed/dict_of_kc.json', 'w') as fp:\n",
    "        json.dump(dict1_kc, fp)\n",
    "        \n",
    "    # Save preprocessed data basic info\n",
    "    with open( folder_name+'/'+ course_name+\"/processed/config.json\", 'w') as f:\n",
    "        f.write(json.dumps({\n",
    "            'n_users': data.user_id.nunique(),\n",
    "            'n_items': data.item_id.nunique(),\n",
    "            'n_skills': Q_mat.shape[1]\n",
    "            }, indent=4))\n",
    "\n",
    "    \n",
    "    return data, Q_mat, listOfKC, dict1_kc, train_set, test_set, skill_names_ids_map_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_processed_data, q_mat, listOfKC, dict_of_kc, train_set, test_set = prepare_sides('data/kdd', 'sides_20_21', \\\n",
    " #                                                                  'SIDES_20_21_6thyear_withspec_training_prepared.csv', \\\n",
    "  #                                                                  'SIDES_20_21_6thyear_withspec_test_prepared.csv',\\\n",
    "   #                                                                 'specialty', 5, True, True, True)\n",
    " #\n",
    " \n",
    " #\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f029317bb4bfbc0fe13e73024792031a493f346c7d494ac5be7432f44fc5f10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
