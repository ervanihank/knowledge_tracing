{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Elo-based Learner Model - SIDES </h1>\n",
    "This is an implementation of Elo rating system. This mdoel estimates students' knowledge state on different specialties based on their performance on attempting questions in these specialty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from prepare_data_SIDES.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import datetime\n",
    "import math\n",
    "import time\n",
    "import import_ipynb\n",
    "import prepare_data_SIDES as sides_pdr\n",
    "#import find_difficulty_with_irt as find_difficulty\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import warnings\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import dataio\n",
    "\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "In many educational applications there is also an asymmetry between correct and incorrect answers,\n",
    "since students learn just by an exposure to an item. An answer to an item is not only evidence of student’s\n",
    "knowledge, but also an opportunity for learning. In such situations we need to perform different updates\n",
    "for correct and incorrect answers. Papouˇsek et al. (2014) and Pel´anek (2015) propose such modification of\n",
    "the Elo rating system under the name “Performance Factor Analysis Extended / Elo” (PFAE)\n",
    "    \"\"\"\n",
    "hyper_params = {\n",
    "    'a_correct' : 1,\n",
    "    'b_correct' : 0.5,\n",
    "    'a_incorrect' : 1,\n",
    "    'b_incorrect' : 0.5,\n",
    "    'a_question' : 2.5, \n",
    "    'b_question' : 0.5,\n",
    "    'a_specialty' : 2.5,\n",
    "    'b_specialty' : 0.5\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uncertainty Function : \n",
    "$$U(n) = \\frac{a}{1 + b \\times n}$$\n",
    "\n",
    "- $ {a}$:constant hyper-parameters determining the starting value\n",
    "- $ b$ :constant hyper-parameters determining the slope of changes\n",
    "- n: number of prior updates on student’s knowledge state or item difficulty\n",
    "\n",
    "\n",
    "An important difference between applications of the Elo rating system in games and education is an\n",
    "asymmetry between students and items in education. For items we expect their difficulty to be approximately\n",
    "constant. In some cases it may be useful to track changes in difficulty, e.g., in geography general knowledge\n",
    "of some places may temporarily change due to their presence in media (Guinea during the Ebola epidemic).\n",
    "But such cases are exceptions and changes in difficulty are not expected to be large. On the other hand,\n",
    "changes in student skill are expected – after all, that is the aim of educational systems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncertainty_k(n, a, b,update_for):\n",
    "    \"\"\"\n",
    "    - Most Elo extensions use an \"uncertainty function\" instead of a constant K.\n",
    "    - The uncertainty function is used to adjust the size of the update based on the amount of data available.\n",
    "    - For new players (students), the skill estimate is highly uncertain, and the update should be larger. As more data becomes available, the size of the update should get smaller.\n",
    "    - Papoušek et al. (2014) and Nižnan et al. (2015) set `a` to 1 and `b` to 0.05 in their uncertainty function.\n",
    "    - Abdi et al. (2019) set `a` to 1.8 and `b` to 0.05 in their experiments on public datasets.\n",
    "    - In educational applications, there is often an asymmetry in the number of available answers for items and students. Each item is answered by many students, whereas for students, the number of answers is typically smaller by orders of magnitude.\n",
    "    - Pelanek et al. (2016) suggest using different uncertainty functions for items and students may be useful in such cases.\n",
    "    - Experience suggests that in the case of student modeling, it may be sufficient to model uncertainty in a simpler, pragmatic way (Nižnan et al., 2015).\n",
    "    \"\"\"\n",
    "    u = a / (1 + (b*n))\n",
    "    #u = 0.1\n",
    "    # set a lower bound on uncertainty if update is for a student and if u<0.03\n",
    "    if update_for == 'student' and np.any(u < 0.03):\n",
    "        u = 0.03\n",
    "    return u"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chance Level\n",
    "If QUA (Question with Unique Answer) and question have ${n}_{opt}$ :\n",
    "\n",
    "$$ P(\\text{guessing}| {n}_{opt}, {type}_{answer})  = \\frac{1} {n_{opt}} $$\n",
    "\n",
    "If QMA (Question qith Multiple Answer) and question have ${n}_{opt}$ :\n",
    "\n",
    "$$P(\\text{guessing}| {n}_{opt}, {type}_{answer})  =  \\frac{1} { \\sum_{k=1}^{n_{opt}} ({n_{opt} \\choose 1} , {n_{opt} \\choose 2} , \\cdots , {n_{opt} \\choose n_{opt}})}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_guessing(n_opt,answer_type):\n",
    "    \"\"\"\n",
    "    Calculates the probability of a student answering a multiple-choice question\n",
    "    (with n_opt options and n_correct_opt correct options) correctly by chance, assuming\n",
    "    that the student can choose any number of options from 1 (see if you want 0 check option into probability?)\n",
    "    to n_opt.\n",
    "    Note that: If the question has only 1 correct answer then the probability is 1/n_opt. \n",
    "    (assuming that students know that they have to choose only 1 option) !!! Check if there is such info for multianswer\n",
    "    \"\"\"\n",
    "    n_combinations = sum([math.comb(n_opt, k) for k in range(1,n_opt + 1)])\n",
    "    p = 1 / (n_opt if answer_type == 'QUA' else n_combinations)\n",
    "    \n",
    "    \n",
    "    return p"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probability that student un answers an item qm correctly : \n",
    "$$P(a_{nm}=1 | \\bar{\\lambda}_{nm}, d_m) = \\sigma(\\bar{\\lambda}_{nm} - d_m)$$\n",
    "\n",
    "- qm : question m tagged with g knowledge components including knowledge component $\\delta_l$\n",
    "- $\\bar{\\lambda}_{nl}$: un’s knowledge state on concept $\\delta_l$\n",
    "- $\\bar{\\lambda}_{nm} = \\sum_{l=1}^L \\lambda_{nl} \\times \\omega_{ml}$  :  un’s average competency on concepts that are associated with qm\n",
    "\n",
    "#### If we add guessing to the probability of answering correctly :\n",
    "\n",
    "$$P(\\text{correct}_{nm} = 1)=P(\\text{guessing}| {n}_{opt}, {type}_{answer}) + \\frac{(1-P(\\text{guessing}| {n}_{opt}, {type}_{answer}))}{1 + e^{-(\\bar{\\lambda}_{nm} - d_m)}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expect_score(student_rating, question_rating, n_options, answer_type):\n",
    "    \"\"\"\n",
    "    Expected score of the interaction for a multiple-choice question with k options.\n",
    "    \"\"\"\n",
    "    #guess_prob = chance_mat.loc[n_correct_options, n_options]\n",
    "    guess_prob = probability_guessing(n_options,answer_type)  # probability of guessing the correct answer\n",
    "    knowledge_prob = 1 - guess_prob  # probability of answering correctly based on knowledge\n",
    "    logistic_input = 1 + np.exp(-(student_rating - question_rating))\n",
    "    return guess_prob + (knowledge_prob / logistic_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expect_score2(student_rating, question_rating, n_options, answer_type, specialty_rating):\n",
    "    \"\"\"\n",
    "    Expected score of the interaction for a multiple-choice question with k options.\n",
    "    \"\"\"\n",
    "    #guess_prob = chance_mat.loc[n_correct_options, n_options]\n",
    "    guess_prob = probability_guessing(n_options,answer_type)  # probability of guessing the correct answer\n",
    "    knowledge_prob = 1 - guess_prob  # probability of answering correctly based on knowledge\n",
    "    logistic_input = 1 + np.exp(-(student_rating - (question_rating+specialty_rating)))\n",
    "    return guess_prob + (knowledge_prob / logistic_input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculating RMSE: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''calculating RMSE (root mean squared error) based on model predictions and actual responses'''\n",
    "def CalculateRMSE(Output, ground, I):\n",
    "    Output = np.array(Output)\n",
    "    ground = np.array(ground)\n",
    "    error = (Output - ground) \n",
    "    err_sqr = error*error\n",
    "    RMSE = math.sqrt(err_sqr.sum()/I)\n",
    "    return RMSE  \n",
    "def auc_roc(y, pred): \n",
    "    y = np.array(y)\n",
    "    pred = np.array(pred)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr) \n",
    "    return auc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Elo \n",
    "\n",
    "##### Update Rules \n",
    "- For Question Difficulty: $$d_m := d_m +  U(n)(\\mathrm{P}(a_{nm}=1|\\bar{\\lambda}{nm}, d_m) - a_{nm})$$\n",
    "- For Student Ability in each tagged specialty : $$\\lambda_{nl} := \\lambda_{nl} +  U(n)(a_{nm} - \\mathrm{P}(a_{nm}=1|\\lambda_{nl}, d_m))$$\n",
    "\n",
    " where $ a_{nm}$ is the real (binary) result and $\\alpha$ is a normalization factor, ensuring that the zero-sum game principles are enforced in the model\n",
    "\n",
    " $\\alpha$ is computed using the following formula:\n",
    "$$\\alpha = \\frac{|\\mathrm{P}(a_{nm}=1|\\bar{\\lambda}{nm}, d{m})-a_{nm}|}{\\sum\\limits_{l=1}^{L}|\\mathrm{a_{nm}}-\\mathrm{P}(a_{nm}=1|\\lambda_{nl},d_{m})\\times\\omega_{ml}|}$$\n",
    "\n",
    "where $ \\omega_{ml}$ is the number of specialties tagged by question m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runelo_item_training(df,hyper_params=hyper_params,include_spec_difficulty=True):\n",
    "    \"\"\"\n",
    "    Implementation of Elo rating system for adaptive educational learning platforms.\n",
    "    This function is used on the training set to learn the difficulty of items. \n",
    "    Once the difficulty of items are learneres by this function, they are used in another training function (runelo_studentability_training)to learn the competency of students. \n",
    "    \n",
    "    Arguments:\n",
    "    df -- train data in the form of Pandas data frame\n",
    "    \n",
    "    Output: data_outputs that contains elo_ExpectedScore and actual_score for each line of df\n",
    "    \"\"\"\n",
    "    # call hyper-parameters\n",
    "    a_correct = hyper_params['a_correct']\n",
    "    b_correct = hyper_params['b_correct']\n",
    "    a_incorrect = hyper_params['a_incorrect']\n",
    "    b_incorrect = hyper_params['b_incorrect']\n",
    "    a_question = hyper_params['a_question']\n",
    "    b_question = hyper_params['b_question']\n",
    "    a_specialty = hyper_params['a_specialty']\n",
    "    b_specialty = hyper_params['b_specialty']\n",
    "    \n",
    "    # Pre-compute values that depend only on the questions\n",
    "    n_tagged_specialty_list = np.count_nonzero(q_mat, axis=1)\n",
    "    wml_list = np.where(n_tagged_specialty_list > 0, 1 / n_tagged_specialty_list, 0)\n",
    "    \n",
    "    print(\"M-Elo execution for learning item difficulty is started.\") \n",
    "    \n",
    "    # Create arrays to store the expected score, actual score, student ID, and question ID for each row in df\n",
    "    n_rows = len(df)\n",
    "    elo_ExpectedScore = np.zeros(n_rows)\n",
    "    actual = np.zeros(n_rows)\n",
    "    student = np.zeros(n_rows, dtype=int)\n",
    "    question=  np.zeros(n_rows, dtype=int)\n",
    "    specialty = np.zeros(n_rows, dtype=object)\n",
    "    question_dif = np.zeros(n_rows)\n",
    "    student_average_ability = np.zeros(n_rows)\n",
    "    specialty_average_difficulty= np.zeros(n_rows)\n",
    "    \n",
    "    # if learn_conf_interval:\n",
    "    #     # Define the number of bootstrap samples\n",
    "    #     num_bootstrap_samples = 1000\n",
    "    #     # Add the 'upperbound' and 'lowerbound' columns with NaN values\n",
    "    #     for item in question_difficulty_updates_list:\n",
    "    #         item['upperbound'] = np.nan\n",
    "    #         item['lowerbound'] = np.nan\n",
    "\n",
    "\n",
    "    #start iteration over rows in df\n",
    "    for count, (index, item) in enumerate(df.iterrows()):\n",
    "        # Step 0: Initialization\n",
    "        uid = item['user_id']\n",
    "        qid = item['item_id']\n",
    "        n_options = item['n_options']\n",
    "        answer_type = item['answer_type']\n",
    "        correct = item['correct']\n",
    "        spec= item['kc_id']\n",
    "        \n",
    "        actual[count] = correct\n",
    "        student[count] = uid\n",
    "        question[count] = qid\n",
    "        specialty[count] = spec\n",
    "\n",
    "        wml = wml_list[qid] # weight for each specialty\n",
    "        d = question_difficulty[qid] #getting the difficulty of the question qid from difficulty matrix\n",
    "        c = learner_competency[uid] * q_mat[qid] # getting the competency of the student on the specialty associated with the question\n",
    "        c_avg = np.sum(c)*wml # weighted average competency of the student on all the specialties associated with the question\n",
    "        \n",
    "        student_average_ability[count] = c_avg\n",
    "        question_dif[count] = d\n",
    "        \n",
    "        topic_mask = q_mat[qid] != 0 ## Create a boolean mask indicating which elements in the q_mat row for the current question are non-zero\n",
    "\n",
    "        \n",
    "        ## Step 1: calculate the expected result from student and item point of view\n",
    "        if include_spec_difficulty:\n",
    "            d_spec= specialty_difficulty[0]*q_mat[qid]\n",
    "            d_spec_avg = np.sum(d_spec)*wml # weighted average competency of the student on all the specialties associated with the question\n",
    "            specialty_average_difficulty[count] = d_spec_avg\n",
    "            # 1-a: calculate the expected outcome and alpha based on proficiency on each topic\n",
    "            u_topic_expected_result = np.where(topic_mask, expect_score2(c, d, n_options, answer_type, d_spec), 0) # expected score is only calculated for topics where the corresponding entry in q_mat is non-zero\n",
    "            # 1-b: calculate the expected outcome based on proficiency on all topics\n",
    "            u_item_expected_result = expect_score2(c_avg, d, n_options,answer_type,d_spec_avg)\n",
    "        else:\n",
    "            # 1-a: calculate the expected outcome and alpha based on proficiency on each topic\n",
    "            u_topic_expected_result = np.where(topic_mask, expect_score(c, d, n_options, answer_type), 0) # expected score is only calculated for topics where the corresponding entry in q_mat is non-zero\n",
    "            # 1-b: calculate the expected outcome based on proficiency on all topics\n",
    "            u_item_expected_result = expect_score(c_avg, d, n_options,answer_type)\n",
    "            \n",
    "        elo_ExpectedScore[count] = u_item_expected_result\n",
    "\n",
    "        # Step 2: calculate normalization factor (alpha) to ensure that the zero-sum game principles are enforced in the model\n",
    "        #alpha_topic = abs(correct - (u_topic_expected_result[topic_mask] * wml))\n",
    "        #sum_alpha_topic = np.sum(alpha_topic)\n",
    "        #if sum_alpha_topic == 0:\n",
    "        #    sum_alpha_topic = 1e-10   \n",
    "        #alpha = (abs(u_item_expected_result - correct)) / sum_alpha_topic\n",
    "\n",
    "        # Step 3: updates\n",
    "        # 3-a: update students ability on each specialty tagged by question\n",
    "        # Vectorize the computation for each student and specialty\n",
    "        n_prev_updates = attempt_counter_student_spec[uid, topic_mask]\n",
    "        if correct == 1:\n",
    "            change = uncertainty_k(n=n_prev_updates, a=a_correct, b=b_correct, update_for='student') * (correct - u_topic_expected_result[topic_mask])\n",
    "        else:\n",
    "            change = uncertainty_k(n=n_prev_updates,a=a_incorrect,b=b_incorrect, update_for='student') * (correct - u_topic_expected_result[topic_mask])\n",
    "        learner_competency[uid, topic_mask] += change\n",
    "        attempt_counter_student_spec[uid, topic_mask] += 1\n",
    "        # Track student ability evolution\n",
    "        mask_indices = np.where(topic_mask)[0]\n",
    "        for spec_idx in mask_indices:\n",
    "            student_ability_updates_list.append({'student': uid, 'specialty': spec_idx, 'ability': learner_competency[uid, spec_idx]})\n",
    "\n",
    "        # # 3-b: update difficulty level of each questions\n",
    "        n_prev_updates_q = attempt_counter_question[qid]\n",
    "        change_question=uncertainty_k(n=n_prev_updates_q, a=a_question, b=b_question,update_for='question') * (u_item_expected_result - correct)\n",
    "        question_difficulty[qid] += change_question\n",
    "        #update attttempt_no\n",
    "        attempt_counter_question[qid] += 1\n",
    "        #track question difficulty evolution\n",
    "        question_difficulty_updates_list.append({'question': qid, 'difficulty': question_difficulty[qid]})\n",
    "        \n",
    "        if include_spec_difficulty:\n",
    "            # # 3-c: update difficulty level of each specialty\n",
    "            n_prev_updates_spec = attempt_counter_spec[0, topic_mask]\n",
    "            change_spec= uncertainty_k(n=n_prev_updates_spec, a=a_specialty, b=b_specialty, update_for='specialty') * (u_topic_expected_result[topic_mask]- correct)\n",
    "            specialty_difficulty[0, topic_mask] += change_spec\n",
    "            #update attttempt_no\n",
    "            attempt_counter_spec[0, topic_mask] += 1\n",
    "            # track specialty difficulty evolution\n",
    "            for spec_idx in mask_indices:\n",
    "                specialty_difficulty_updates_list.append({'specialty': spec_idx, 'difficulty': specialty_difficulty[0, spec_idx]})\n",
    "            \n",
    "        \n",
    "        # if learn_conf_interval:\n",
    "        #     # Step 4: bootstrapping\n",
    "        #     # Convert the question_difficulty_updates_list to a DataFrame\n",
    "        #     df_question_difficulty_updates = pd.DataFrame(question_difficulty_updates_list)\n",
    "        #     # Filter the DataFrame based on the question ID\n",
    "        #     question_difficulty_update_qid = df_question_difficulty_updates[df_question_difficulty_updates['question'] == qid]\n",
    "        #     # Perform bootstrapping using the resample function from scikit-learn\n",
    "        #     bootstrapped_difficulty = resample(question_difficulty_update_qid['difficulty'], replace=True, n_samples=num_bootstrap_samples)\n",
    "        #     # Calculate the 95% confidence interval for the bootstrapped difficulty values\n",
    "        #     lower_bound = np.percentile(bootstrapped_difficulty, 2.5)\n",
    "        #     upper_bound = np.percentile(bootstrapped_difficulty, 97.5)\n",
    "        #     # Update the upperbound and lowerbound columns in question_difficulty_updates_list but only for the last appended row\n",
    "        #     # Update the 'lowerbound' and 'upperbound' values in the last appended row\n",
    "        #     question_difficulty_updates_list[-1]['lowerbound'] = lower_bound\n",
    "        #     question_difficulty_updates_list[-1]['upperbound'] = upper_bound\n",
    "\n",
    "    print(\"M-Elo execution for learning item difficulty is ended.\")\n",
    "    # create a dictionary with column names as keys and lists as values\n",
    "    data_outputs = {'student': student,'question': question, 'specialty': specialty, 'question_dif': question_dif, 'student_average_ability': student_average_ability, 'specialty_average_difficulty':specialty_average_difficulty, 'actual_score': actual, 'elo_ExpectedScore': elo_ExpectedScore }\n",
    "\n",
    "    return data_outputs # later, delete returning values from this function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runelo_studentability_training(df,hyper_params=hyper_params,include_spec_difficulty=True):\n",
    "\n",
    "    \"\"\"\n",
    "    Implementation of Elo rating system for adaptive educational learning platforms.\n",
    "    This function is used to update the competency of students based on the difficulty of items obtained\n",
    "    from runelo_item_training function. \n",
    "    !! Please be advised that in this function, the difficulty of items do not change and only the competency of students are changing. \n",
    "    \n",
    "    Arguments:\n",
    "    df -- train data in the form of Pandas data frame\n",
    "    \n",
    "    Output:\n",
    "    \"\"\"\n",
    "    # call hyper-parameters\n",
    "    a_correct = hyper_params['a_correct']\n",
    "    b_correct = hyper_params['b_correct']\n",
    "    a_incorrect = hyper_params['a_incorrect']\n",
    "    b_incorrect = hyper_params['b_incorrect']\n",
    "    \n",
    "    # Pre-compute values that depend only on the questions\n",
    "    n_tagged_specialty_list = np.count_nonzero(q_mat, axis=1)\n",
    "    wml_list = np.where(n_tagged_specialty_list > 0, 1 / n_tagged_specialty_list, 0)\n",
    "    \n",
    "    print(\"M-Elo execution for estimating student competency is started.\") \n",
    "    \n",
    "    # Create arrays to store the expected score, actual score, student ID, and question ID for each row in df\n",
    "    n_rows = len(df)\n",
    "    elo_ExpectedScore = np.zeros(n_rows)\n",
    "    actual = np.zeros(n_rows)\n",
    "    student = np.zeros(n_rows, dtype=int)\n",
    "    question=  np.zeros(n_rows, dtype=int)\n",
    "    specialty = np.zeros(n_rows, dtype=object)\n",
    "    question_dif = np.zeros(n_rows)\n",
    "    student_average_ability = np.zeros(n_rows)\n",
    "    specialty_average_difficulty= np.zeros(n_rows)\n",
    "    \n",
    "    for count, (index, item) in enumerate(df.iterrows()):\n",
    "        # Step 0: Initialization\n",
    "        uid = item['user_id']\n",
    "        qid = item['item_id']\n",
    "        n_options = item['n_options']\n",
    "        answer_type = item['answer_type']\n",
    "        correct = item['correct']\n",
    "        spec= item['kc_id']\n",
    "\n",
    "        \n",
    "        actual[count] = correct\n",
    "        student[count] = uid\n",
    "        question[count] = qid\n",
    "        specialty[count] = spec\n",
    "        \n",
    "        wml = wml_list[qid]\n",
    "        d = question_difficulty[qid] #getting the difficulty of the question qid from difficulty matrix\n",
    "        c = learner_competency[uid] * q_mat[qid] # getting the competency of the student on the topics associated with the question\n",
    "        c_avg = np.sum(c)*wml # weighted average competency of the student on the topics associated with the question\n",
    "\n",
    "        student_average_ability[count] = c_avg\n",
    "        question_dif[count] = d\n",
    "        \n",
    "        topic_mask = q_mat[qid] != 0 ## Create a boolean mask indicating which elements in the q_mat row for the current question are non-zero\n",
    "        \n",
    "        ## Step 1: calculate the expected result from student and item point of view\n",
    "        if include_spec_difficulty:\n",
    "            d_spec= specialty_difficulty[0]*q_mat[qid]\n",
    "            d_spec_avg = np.sum(d_spec)*wml # weighted average competency of the student on all the specialties associated with the question\n",
    "            specialty_average_difficulty[count] = d_spec_avg\n",
    "            # 1-a: calculate the expected outcome and alpha based on proficiency on each topic\n",
    "            u_topic_expected_result = np.where(topic_mask, expect_score2(c, d, n_options, answer_type, d_spec), 0) # expected score is only calculated for topics where the corresponding entry in q_mat is non-zero\n",
    "            # 1-b: calculate the expected outcome based on proficiency on all topics\n",
    "            u_item_expected_result = expect_score2(c_avg, d, n_options,answer_type,d_spec_avg)\n",
    "        else:\n",
    "            # 1-a: calculate the expected outcome and alpha based on proficiency on each topic\n",
    "            u_topic_expected_result = np.where(topic_mask, expect_score(c, d, n_options, answer_type), 0) # expected score is only calculated for topics where the corresponding entry in q_mat is non-zero\n",
    "            # 1-b: calculate the expected outcome based on proficiency on all topics\n",
    "            u_item_expected_result = expect_score(c_avg, d, n_options,answer_type)\n",
    "            \n",
    "        elo_ExpectedScore[count] = u_item_expected_result\n",
    "\n",
    "        # Step 2: calculate normalization factor (alpha) to ensure that the zero-sum game principles are enforced in the model\n",
    "        #alpha_topic = abs(correct - (u_topic_expected_result[topic_mask] * wml))\n",
    "        #sum_alpha_topic = np.sum(alpha_topic)\n",
    "        #if sum_alpha_topic == 0:\n",
    "        #    sum_alpha_topic = 1e-10  \n",
    "        #alpha = abs(u_item_expected_result - correct) / sum_alpha_topic\n",
    "\n",
    "        # Step 3: updates. here no need to update the difficulty of the question and specialty\n",
    "        # 3-a: update students ability on each specialty tagged by question\n",
    "        # Vectorize the computation for each student and specialty\n",
    "        n_prev_updates = attempt_counter_student_spec[uid, topic_mask]\n",
    "        if correct == 1:\n",
    "            change = uncertainty_k(n=n_prev_updates, a=a_correct, b=b_correct,update_for='student') * (correct - u_topic_expected_result[topic_mask])\n",
    "        else:\n",
    "            change = uncertainty_k(n=n_prev_updates,a=a_incorrect,b=b_incorrect,update_for='student') * (correct - u_topic_expected_result[topic_mask])\n",
    "            \n",
    "        learner_competency[uid, topic_mask] += change\n",
    "        attempt_counter_student_spec[uid, topic_mask] += 1\n",
    "\n",
    "        # Track student ability evolution\n",
    "        mask_indices = np.where(topic_mask)[0]\n",
    "        for spec_idx in mask_indices:\n",
    "            student_ability_updates_list.append({'student': uid, 'specialty': spec_idx, 'ability': learner_competency[uid, spec_idx]})\n",
    "    \n",
    "    print(\"M-Elo execution for estimating student competency is ended.\")\n",
    "    # create a dictionary with column names as keys and lists as values\n",
    "    data_outputs = {'student': student,'question': question, 'specialty': specialty, 'question_dif': question_dif, 'student_average_ability': student_average_ability, 'specialty_average_difficulty':specialty_average_difficulty, 'actual_score': actual, 'elo_ExpectedScore': elo_ExpectedScore }\n",
    "    \n",
    "    return data_outputs  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funtion for mapping the question difficulty found by logreg to the elo difficulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question_difficulty_form_previous_year(question_difficulty_irt, old_new_item_ids_irt, old_new_item_ids_elo, question_difficulty,question_difficulty_updates_list,attempt_counter_question_irt,attempt_counter_question, count_attemp=True, reverse_coef=True):\n",
    "        \n",
    "        # multiply difficulty_irt column with -1 if reverse_coef is True\n",
    "        if reverse_coef:\n",
    "                question_difficulty_irt['difficulty_irt'] = question_difficulty_irt['difficulty_irt'] * -1\n",
    "        # fill the question_difficulty with the question_difficulty found by irt\n",
    "        # merge the two dataframes on the column 'item_id'\n",
    "        question_difficulty_irt = pd.merge(question_difficulty_irt, old_new_item_ids_irt, on='item_id')\n",
    "        # remove the column 'item_id'\n",
    "        question_difficulty_irt = question_difficulty_irt.drop(columns=['item_id'])\n",
    "        # merge the two dataframes on the 'question' column\n",
    "        merged = pd.merge(old_new_item_ids_elo, question_difficulty_irt, on='question', how='left')\n",
    "        #remove the column 'question'\n",
    "        merged = merged.drop(columns=['question'])\n",
    "        # remove the rows that have nan values in the column 'difficulty_irt'\n",
    "        merged = merged.dropna(subset=['difficulty_irt'])\n",
    "        # fill the question_difficulty array with the item difficulties found by irt\n",
    "        item_numbers= merged['item_id'].values\n",
    "        difficulties = merged['difficulty_irt'].values\n",
    "        question_difficulty[item_numbers] = difficulties\n",
    "        \n",
    "        \n",
    "        # fill the question_difficulty_updates_list with the item difficulties found by irt\n",
    "        # create a dictionary with item_id as key and irt difficulty as value\n",
    "        item_difficulty_dict = dict(zip(merged['item_id'], merged['difficulty_irt']))\n",
    "        # replace the 0 in question_difficulty_updates_list with the corresponding difficulty_irt\n",
    "        for item in question_difficulty_updates_list:\n",
    "                if item['question'] in item_difficulty_dict:\n",
    "                        item['difficulty'] = item_difficulty_dict[item['question']]\n",
    "                        \n",
    "        \n",
    "        if count_attemp:\n",
    "                # fill the attempt_counter_question with the attempt_counter found by irt\n",
    "                # merge attempt_counter_question_irt with attempt_counter_question\n",
    "                attempt_counter_question_irt = pd.merge(attempt_counter_question_irt, old_new_item_ids_irt, on='item_id')\n",
    "                # remove the column 'item_id'\n",
    "                attempt_counter_question_irt = attempt_counter_question_irt.drop(columns=['item_id'])\n",
    "                # merge old_new_item_ids_elo with attempt_counter_question_irt\n",
    "                merged = pd.merge(old_new_item_ids_elo, attempt_counter_question_irt, on='question', how='left')\n",
    "                # remove the column 'question'\n",
    "                merged = merged.drop(columns=['question'])\n",
    "                # remove the rows that have nan values in the column 'attempt_counter'\n",
    "                merged = merged.dropna(subset=['count'])\n",
    "                # fill the attempt_counter_question array with the attempt_counter found by irt\n",
    "                item_numbers= merged['item_id'].values\n",
    "                attempt_counter = merged['count'].values\n",
    "                attempt_counter_question[item_numbers] = attempt_counter\n",
    "                  \n",
    "        return question_difficulty, question_difficulty_updates_list, attempt_counter_question"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funtion for mapping the student ability found by logreg to the elo student ability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ability(merged_dict, user_id, specialty):\n",
    "    user_ids = merged_dict['user_id']\n",
    "    specialties = merged_dict['specialty']\n",
    "    abilities = merged_dict['ability']\n",
    "\n",
    "    if user_id in user_ids and specialty in specialties:\n",
    "        user_index = user_ids.index(user_id)\n",
    "        specialty_index = specialties.index(specialty)\n",
    "        ability = abilities[user_index][specialty_index]\n",
    "        return ability\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_user_ability_form_previous_year(stundet_ability_irt, old_new_user_ids_irt, old_new_user_ids_elo, learner_competency,student_ability_updates_list, attempt_counter_student_spec_irt, attempt_counter_student_spec,old_new_skill,count_attemp=True, reverse_coef=False):\n",
    "        \n",
    "        # multiply the columns from 1 to end of the stundet_ability_irt by -1  if reverse_coef is True\n",
    "        if reverse_coef:\n",
    "                stundet_ability_irt.iloc[:, 1:] = stundet_ability_irt.iloc[:, 1:] * -1\n",
    "                \n",
    "        # merge the two dataframes on the column 'item_id'\n",
    "        stundet_ability_irt = pd.merge(stundet_ability_irt, old_new_user_ids_irt, on='user_id')        # remove the column 'item_id'\n",
    "        stundet_ability_irt = stundet_ability_irt.drop(columns=['user_id'])\n",
    "        \n",
    "        # merge the two dataframes on the 'question' column\n",
    "        merged = pd.merge(old_new_user_ids_elo, stundet_ability_irt, on='student', how='left')\n",
    "        #remove the column 'question'\n",
    "        merged = merged.drop(columns=['student'])\n",
    "        \n",
    "        # remove the rows that have nan values in any column \n",
    "        merged = merged.dropna()\n",
    "        \n",
    "        # turn old_new_skill into an array\n",
    "        # Convert the array to a DataFrame\n",
    "        old_new_skill = pd.DataFrame(old_new_skill)\n",
    "        # Rename the column as 'skill_name'\n",
    "        old_new_skill = old_new_skill.rename(columns={0: 'skill_name'})\n",
    "\n",
    "        # Reset the index and rename the index column\n",
    "        old_new_skill = old_new_skill.reset_index().rename(columns={'index': 'skill_id'})\n",
    "        \n",
    "        # Create a dictionary mapping skill_names to skill_ids\n",
    "        skill_mapping = old_new_skill.set_index('skill_name')['skill_id'].to_dict()\n",
    "\n",
    "        # create a dictionary to map the column names of merged to the corresponding skill_id\n",
    "        dict_user={'used_id':'user_id'}\n",
    "        # join a and skill_mapping\n",
    "        dict_user.update(skill_mapping)\n",
    "        \n",
    "        merged = merged.rename(columns=skill_mapping)\n",
    "        \n",
    "        # turn merged unto dictionary\n",
    "        merged_dict = {\n",
    "        'user_id': merged['user_id'].tolist(),\n",
    "        'specialty': merged.columns[1:].tolist(),\n",
    "        'ability': merged.iloc[:, 1:].values.tolist()}\n",
    "        \n",
    "        # fill the learner_competency\n",
    "        for i in range(len(merged_dict['user_id'])):\n",
    "                for j in range(len(merged_dict['specialty'])):\n",
    "                        learner_competency[merged_dict['user_id'][i]][merged_dict['specialty'][j]] = merged_dict['ability'][i][j]\n",
    "                        \n",
    "        # Update 'ability' values in student_ability_updates_list\n",
    "        for update in student_ability_updates_list:\n",
    "                if update['student'] in merged_dict['user_id'] and update['specialty'] in merged_dict['specialty']:\n",
    "                        student = update['student']\n",
    "                        specialty = update['specialty']\n",
    "                        ability=find_ability(merged_dict, student, specialty)\n",
    "                        update['ability'] = ability\n",
    "        \n",
    "        if count_attemp:\n",
    "                # Update 'attempt' values in attempt_counter_student_spec\n",
    "                attempt_counter_student_spec_irt = pd.merge(attempt_counter_student_spec_irt, old_new_user_ids_irt, on='user_id')\n",
    "                # remove the column 'item_id'\n",
    "                attempt_counter_student_spec_irt = attempt_counter_student_spec_irt.drop(columns=['user_id'])\n",
    "                # merge old_new_item_ids_elo with attempt_counter_question_irt\n",
    "                merged = pd.merge(old_new_user_ids_elo, attempt_counter_student_spec_irt, on='student', how='left')\n",
    "                # remove the column 'question'\n",
    "                merged = merged.drop(columns=['student'])\n",
    "                # remove the rows that have nan values in the column 'attempt_counter'\n",
    "                merged = merged.dropna()\n",
    "                \n",
    "        \n",
    "        merged = merged.rename(columns=skill_mapping)\n",
    "        \n",
    "        # turn merged unto dictionary\n",
    "        merged_dict = {\n",
    "        'user_id': merged['user_id'].tolist(),\n",
    "        'specialty': merged.columns[1:].tolist(),\n",
    "        'attempt': merged.iloc[:, 1:].values.tolist()}\n",
    "        \n",
    "        # fill the learner_competency\n",
    "        for i in range(len(merged_dict['user_id'])):\n",
    "                for j in range(len(merged_dict['specialty'])):\n",
    "                        attempt_counter_student_spec[merged_dict['user_id'][i]][merged_dict['specialty'][j]] = merged_dict['attempt'][i][j]\n",
    "                        \n",
    "\n",
    "        return learner_competency, student_ability_updates_list, attempt_counter_student_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funtion for mapping the specialty difficulty found by logreg to the elo specialty difficulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spec_difficulty_form_previous_year(specialty_difficulty_irt, old_new_skill, attempt_counter_spec_irt, specialty_difficulty, specialty_difficulty_updates_list, attempt_counter_spec, count_attemp=True, reverse_coef=True):\n",
    "\n",
    "    if reverse_coef:\n",
    "        specialty_difficulty_irt['specialty_difficulty'] = specialty_difficulty_irt['specialty_difficulty'] * -1\n",
    "\n",
    "    # fill the specialty_difficulty with the specialty_difficulty found by irt\n",
    "    # turn old_new_skill into an array\n",
    "    old_new_skill_df = pd.DataFrame(old_new_skill)\n",
    "    # rename the column of old_new_skill as 'specialty'\n",
    "    old_new_skill_df = old_new_skill_df.rename(columns={0: 'specialty'})\n",
    "    # rename the index of old_new_skill as 'specialty_id'\n",
    "    old_new_skill_df = old_new_skill_df.reset_index().rename(columns={'index': 'specialty_id'})\n",
    "    # merge the two dataframes on the 'specialty' column\n",
    "    merged = pd.merge(old_new_skill_df, specialty_difficulty_irt, on='specialty', how='left')\n",
    "    # also merge attempt_counter_specialty_irt with merged on the 'specialty' column\n",
    "    merged = pd.merge(merged, attempt_counter_spec_irt, on='specialty', how='left')\n",
    "    # remove nan values\n",
    "    merged = merged.dropna(subset=['specialty_difficulty'])\n",
    "    # put the values of 'specialty_difficulty' column to the corresponding row in specialty_difficulty\n",
    "    specialty_numbers= merged['specialty_id'].values\n",
    "    difficulties = merged['specialty_difficulty'].values\n",
    "    specialty_difficulty[specialty_numbers] = difficulties\n",
    "\n",
    "    # fill the specialty_difficulty_updates_list with the specialty_difficulty found by irt\n",
    "    if count_attemp:\n",
    "        # put the values of 'attempt_counter' column to the corresponding row in attempt_counter_spec\n",
    "        attempt_counter_spec[specialty_numbers] = merged['total_attempts'].values\n",
    "        \n",
    "    # fill the specialty_difficulty_updates_list with the specialty_difficulty found by irt\n",
    "    # create a dictionary with specialty_id as key and irt difficulty as value\n",
    "    specialty_difficulty_dict = dict(zip(merged['specialty_id'], merged['specialty_difficulty']))\n",
    "    # replace the 0 in specialty_difficulty_updates_list with the corresponding difficulty_irt\n",
    "    for item in specialty_difficulty_updates_list:\n",
    "            if item['specialty'] in specialty_difficulty_dict:\n",
    "                    item['difficulty'] = specialty_difficulty_dict[item['specialty']]\n",
    "                    \n",
    "    return specialty_difficulty, specialty_difficulty_updates_list, attempt_counter_spec\n",
    "    \n",
    "            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model on Data ##"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define HyperParameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### to be able to learn from data, it needs to have enough data available. As our analysis and previous experience shows, the system needs at least 100 students to get good estimates of item difficulty (Pelanek,2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "configurations = {\n",
    "    '2020-2021' : {\n",
    "    'folder' : 'data/sides',\n",
    "    'course' : '2020-2021',\n",
    "    'min_interactions_per_user' : 100,\n",
    "    'min_answer_per_question' : 100,\n",
    "    'kc_column' : 'specialty', \n",
    "    'train_file' : 'train_data.csv', \n",
    "    'test_file' : 'test_data.csv',\n",
    "    'initial_points_question_logreg': True,\n",
    "    'initial_points_students_logreg': True,\n",
    "    'include_spec_difficulty': True,\n",
    "    'initial_points_specdifficulty_logreg': True,\n",
    "    'logreg_year': '2019-2020',\n",
    "    'logreg_model': 'user_skill_together-spec_difficulty-items-weighted_encoding',\n",
    "    'already_preprocessed': True,\n",
    "    'first_go': True,\n",
    "    'second_go': True\n",
    "    }\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DETERMINE THE COURSE ID IN THE FOLLOWING CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dettermine course_id\n",
    "course_id = '2020-2021'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = configurations[course_id]['folder']\n",
    "course = configurations[course_id]['course']\n",
    "min_interactions_per_user = configurations[course_id]['min_interactions_per_user']\n",
    "min_answer_per_question = configurations[course_id]['min_answer_per_question']\n",
    "kc_column = configurations[course_id]['kc_column']\n",
    "train_file = configurations[course_id]['train_file']\n",
    "test_file = configurations[course_id]['test_file']\n",
    "initial_points_question_logreg=configurations[course_id]['initial_points_question_logreg']\n",
    "initial_points_students_logreg=configurations[course_id]['initial_points_students_logreg']\n",
    "include_spec_difficulty=configurations[course_id]['include_spec_difficulty']\n",
    "initial_points_specdifficulty_logreg=configurations[course_id]['initial_points_specdifficulty_logreg']\n",
    "already_preprocessed=configurations[course_id]['already_preprocessed']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save results seperately for seperate versions of the model & save features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features have been written to the file: features.txt in the  data/sides/2020-2021/result_elo_specdif_logreg-q_logreg-skill_logreg-1st-2nd/\n"
     ]
    }
   ],
   "source": [
    "all_features = []\n",
    "\n",
    "if configurations[course_id]['include_spec_difficulty']:\n",
    "    all_features.append('specdif_logreg' if configurations[course_id]['initial_points_specdifficulty_logreg'] else 'specdif_0')\n",
    "\n",
    "all_features.append('q_logreg' if configurations[course_id]['initial_points_question_logreg'] else 'q_0')\n",
    "all_features.append('skill_logreg' if configurations[course_id]['initial_points_students_logreg'] else 'skill_0')\n",
    "\n",
    "all_features.extend(['1st', '2nd'] if configurations[course_id]['first_go'] and configurations[course_id]['second_go'] else [])\n",
    "\n",
    "additional_suffix = '-'.join(all_features)\n",
    "\n",
    "EXPERIMENT_FOLDER = f\"{folder}/{course_id}/result_elo_{additional_suffix}/\"\n",
    "\n",
    "dataio.prepare_folder(EXPERIMENT_FOLDER)\n",
    "\n",
    "# Save the configurations and hyper_params to a TXT file\n",
    "\n",
    "# Combine configurations and hyper_params into a single dictionary\n",
    "combined_dict = {\n",
    "    'configurations': configurations[course_id],\n",
    "    'hyper_params': hyper_params\n",
    "}\n",
    "\n",
    "# Write the configurations dictionary to the TXT file\n",
    "with open(EXPERIMENT_FOLDER + 'features.txt', 'w') as file:\n",
    "    json.dump(combined_dict, file, indent=4)\n",
    "\n",
    "print(\"Features have been written to the file: features.txt in the \", EXPERIMENT_FOLDER)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing the Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### If data already preprocessed call directly (uncomment the second cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already preprocessed. Reading preprocessed data...\n"
     ]
    }
   ],
   "source": [
    "if already_preprocessed and os.path.exists(folder+'/'+ course +\"/processed/preprocessed_data.csv\"):\n",
    "    # print message\n",
    "    print(\"Data already preprocessed. Reading preprocessed data...\")\n",
    "    # read preprocessed data\n",
    "    pre_processed_data = pd.read_csv(folder+'/'+ course +\"/processed/preprocessed_data.csv\")\n",
    "    # read q-matrix\n",
    "    q_mat =q_mat = sparse.load_npz(folder + '/' + course + \"/processed/q_mat.npz\").toarray()\n",
    "    # read listOfKC.json\n",
    "    with open(folder + '/' + course + \"/processed/listOfKC.json\") as json_file:\n",
    "        listOfKC = json.load(json_file)\n",
    "    with open(folder + '/' + course + \"/processed/dict_of_kc.json\") as json_file:\n",
    "        dict_of_kc = json.load(json_file)\n",
    "        \n",
    "    # read train_set\n",
    "    train_set = pd.read_csv(folder + '/' + course + \"/processed/train_set.csv\")\n",
    "    # read test_set\n",
    "    test_set = pd.read_csv(folder + '/' + course + \"/processed/test_set.csv\")\n",
    "    # read skill_names_ids_map_df\n",
    "    skill_names_ids_map_df = pd.read_csv(folder + '/' + course + \"/processed/skill_names_ids_map.csv\")\n",
    "\n",
    "else: # if data is not preprocessed\n",
    "    # print message\n",
    "    print(\"Data not preprocessed. Preprocessing data...\")\n",
    "    # import warnings\n",
    "    warnings.filterwarnings(action='once')\n",
    "    # processing the row data made available by KDD organisers\n",
    "    pre_processed_data, q_mat, listOfKC, dict_of_kc, train_set, test_set,skill_names_ids_map_df = sides_pdr.prepare_sides(folder, course, \\\n",
    "                                                                       train_file, \\\n",
    "                                                                       test_file,\\\n",
    "                                                                        kc_column, min_interactions_per_user, min_answer_per_question,\\\n",
    "                                                                        True, True, True,True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Reading Train and Test sets.\")\n",
    "# train_set = pd.read_csv(folder + '/'+course+\"/processed/train_set.csv\")\n",
    "# test_set = pd.read_csv(folder + '/'+course+\"/processed/test_set.csv\")\n",
    "# pre_processed_data = pd.read_csv(folder + '/'+course+\"/processed/preprocessed_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train set is: (17904661, 8)\n",
      "Shape of test set is: (4387302, 8)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of train set is:\", train_set.shape)\n",
    "print(\"Shape of test set is:\", test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of students is: 8590\n",
      "Number of questions is: 74703\n",
      "Number of specialty is: 31\n"
     ]
    }
   ],
   "source": [
    "# number of students\n",
    "uSize = pre_processed_data['user_id'].nunique()\n",
    "print(\"Number of students is:\", uSize)\n",
    "\n",
    "qSize = pre_processed_data['item_id'].nunique()\n",
    "print(\"Number of questions is:\", qSize)\n",
    "\n",
    "tSize = len(listOfKC)\n",
    "print(\"Number of specialty is:\", tSize)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define chance_mat (guessing probability matrix) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first create a new dataframe with only the unique combinations of 'n_opt' and 'n_correct_options'\n",
    "unique_combinations = pre_processed_data[['n_options', 'answer_type']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Apply probability_guessing function to unique combinations\n",
    "unique_combinations['probability_guessing'] = unique_combinations.apply(\n",
    "    lambda row: probability_guessing(int(row['n_options']), row['answer_type']), axis=1)\n",
    "\n",
    "chance_mat = pd.pivot_table(unique_combinations, values='probability_guessing', index=['answer_type'], columns=['n_options'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning question difficulty from LogReg.\n",
      "Reading question difficulty from file.\n",
      "Learning student ability from LogReg.\n",
      "Reading question difficulty from file.\n",
      "specialty difficulty is included.\n",
      "Learning specialty difficulty from LogReg.\n",
      "Reading specialty difficulty from file.\n"
     ]
    }
   ],
   "source": [
    "question_difficulty = np.zeros(qSize) #stores difficulty level of each question after each answer\n",
    "learner_competency = np.zeros((uSize, tSize)) #stores student's profiency level on each topic and is updated upon each attempt\n",
    "unique_questions = pre_processed_data['item_id'].unique()\n",
    "question_difficulty_updates_list = [{'question': qid, 'difficulty': 0} for qid in unique_questions]\n",
    "attempt_counter_question = np.zeros(qSize)\n",
    "attempt_counter_student_spec=np.zeros((uSize, tSize))\n",
    "\n",
    "'''\n",
    "For students, we create a dataframe, in which index column corresponds\n",
    "to student_id and columns correspond to the topic of the course. It is \n",
    "first populated by all zeros and then it is updated accordingly. \n",
    "If the attempt_counter_student_spec is zero, it shows that it is the first time it has\n",
    "been attempted. Otherwise, the attempt_counter_student_spec should be updated accordingly.\n",
    "'''\n",
    "\n",
    "unique_students = pre_processed_data['user_id'].unique()\n",
    "specialties = list(dict_of_kc.values())\n",
    "student_ability_updates_list = [{'student': uid, 'specialty': spec, 'ability': 0} for uid in unique_students for spec in specialties]\n",
    "\n",
    "# learn question difficulty and student ability from irt if initial_points_irt is True\n",
    "if initial_points_question_logreg: # if initial_points_irt is True (this is defined in the configurations dictionary)\n",
    "    print(\"Learning question difficulty from LogReg.\")\n",
    "    # print that its needed to learn question difficulty from irt by runnin irt_for_large_data.ipynb for the irt_year defined in the configurations dictionary\n",
    "    if not os.path.exists(folder + '/'+ configurations[course_id]['logreg_year']+\"/result_logreg_\" + configurations[course_id]['logreg_model']+ \"/question_difficulty.csv\"):\n",
    "        print(\"Please run loistic regression for the year \"+configurations[course_id]['logreg_year']+\" to learn question difficulty from IRT. and rerun this cell.\")\n",
    "    #    # learn question difficulty from irt by calling the difficulties_irt function from find_difficulty_with_irt.py\n",
    "    #    question_difficulty = find_difficulty.difficulties_irt(train_set, q_mat, question_difficulty, StudPerQuest=100, QuestPerStud=100, HighQuestNb=True)\n",
    "    #    # save the question difficulty in a csv file\n",
    "    #    np.savetxt(folder + '/'+course+\"/processed/question_difficulty_irt.csv\", question_difficulty, delimiter=\",\")\n",
    "    \n",
    "    else:\n",
    "        print(\"Reading question difficulty from file.\")\n",
    "        # read item_deltas.csv file\n",
    "        question_difficulty_irt = pd.read_csv(folder + '/'+ configurations[course_id]['logreg_year']+\"/result_logreg_\" + configurations[course_id]['logreg_model']+ \"/question_difficulty.csv\")\n",
    "        old_new_item_ids_irt= pd.read_csv(folder + '/'+ configurations[course_id]['logreg_year']+\"/processed\"+\"/old_new_item_ids.csv\")\n",
    "        old_new_item_ids_elo = pd.read_csv(folder + '/'+course+\"/processed/old_new_item_ids.csv\") \n",
    "        attempt_counter_question_irt= pd.read_csv(folder + '/'+ configurations[course_id]['logreg_year']+\"/processed\"+\"/attempt_counter_question.csv\")\n",
    "        # get question difficulty and question_difficulty_updates_list from get_question_difficulty_form_previous_year function\n",
    "        question_difficulty, question_difficulty_updates_list , attempt_counter_question = get_question_difficulty_form_previous_year(question_difficulty_irt, old_new_item_ids_irt, old_new_item_ids_elo, question_difficulty, question_difficulty_updates_list,attempt_counter_question_irt,attempt_counter_question,count_attemp=False, reverse_coef=True)\n",
    "else:\n",
    "    print(\"Learning question difficulty from ELO directly.\")\n",
    "\n",
    "# learn question difficulty and student ability from irt if initial_points_irt is True\n",
    "if initial_points_students_logreg: # if initial_points_irt is True (this is defined in the configurations dictionary)\n",
    "    print(\"Learning student ability from LogReg.\")\n",
    "    # print that its needed to learn question difficulty from irt by runnin irt_for_large_data.ipynb for the irt_year defined in the configurations dictionary\n",
    "    if not os.path.exists(folder + '/'+ configurations[course_id]['logreg_year']+\"/result_logreg_\" + configurations[course_id]['logreg_model']+\"/learner_competency.csv\"):\n",
    "        print(\"Please run logistic regressin for the year \"+configurations[course_id]['logreg_year']+\" to learn user ability from IRT. and rerun this cell.\")\n",
    "\n",
    "    else:\n",
    "        if not os.path.exists(folder + '/'+ configurations[course_id]['logreg_year']+\"/processed/attempt_counter_student_spec.csv\"):\n",
    "            print(\"Please run logistic reression for the year \"+configurations[course_id]['logreg_year']+\" to learn attempt_counter_student_spec from IRT. and rerun this cell.\")\n",
    "        else:\n",
    "            print(\"Reading question difficulty from file.\")\n",
    "            # read item_deltas.csv file\n",
    "            stundet_ability_irt = pd.read_csv(folder + '/'+ configurations[course_id]['logreg_year']+\"/result_logreg_\" + configurations[course_id]['logreg_model']+\"/learner_competency.csv\")\n",
    "            old_new_user_ids_irt= pd.read_csv(folder + '/'+ configurations[course_id]['logreg_year']+\"/processed\"+\"/old_new_user_ids.csv\")\n",
    "            attempt_counter_student_spec_irt= pd.read_csv(folder + '/'+ configurations[course_id]['logreg_year']+\"/processed\"+\"/attempt_counter_student_spec.csv\")\n",
    "            old_new_user_ids_elo = pd.read_csv(folder + '/'+course+\"/processed/old_new_user_ids.csv\")\n",
    "            old_new_skill= listOfKC.copy()\n",
    "            # get question difficulty and question_difficulty_updates_list from get_question_difficulty_form_previous_year function\n",
    "            learner_competency, student_ability_updates_list, attempt_counter_student_spec = get_user_ability_form_previous_year(stundet_ability_irt, old_new_user_ids_irt, old_new_user_ids_elo, learner_competency,student_ability_updates_list, attempt_counter_student_spec_irt, attempt_counter_student_spec,old_new_skill,count_attemp=False, reverse_coef=False)\n",
    "else:\n",
    "    print(\"Learning student ability from ELO directly.\")\n",
    "    \n",
    "if include_spec_difficulty:\n",
    "    print(\"specialty difficulty is included.\")\n",
    "    # open a specialty_difficulty array with specialty names from listOfKC in the first column and zeros in the second column\n",
    "    specialty_difficulty = np.zeros((len(specialties)))\n",
    "    specialty_difficulty_updates_list = [{'specialty': spec, 'difficulty': 0} for spec in specialties]\n",
    "    attempt_counter_spec=np.zeros((len(specialties)))\n",
    "    \n",
    "    # learn specialty difficulty from irt if initial_points_specdifficulty_logreg is True\n",
    "    if initial_points_specdifficulty_logreg:\n",
    "        print(\"Learning specialty difficulty from LogReg.\")\n",
    "        # print that its needed to learn question difficulty from irt by runnin irt_for_large_data.ipynb for the irt_year defined in the configurations dictionary\n",
    "        if not os.path.exists(folder + '/'+ configurations[course_id]['logreg_year']+\"/result_logreg_\" + configurations[course_id]['logreg_model']+\"/specialty_difficulty.csv\"):\n",
    "            print(\"Please run irt_for_large_data.ipynb for the year \"+configurations[course_id]['logreg_year']+\" to learn specialty difficulty from IRT. and rerun this cell.\")\n",
    "            \n",
    "        else:\n",
    "            if not os.path.exists(folder + '/'+ configurations[course_id]['logreg_year']+\"/processed/attempt_counter_spec.csv\"):\n",
    "                print(\"Please run irt_for_large_data.ipynb for the year \"+configurations[course_id]['logreg_year']+\" to learn attempt_counter_spec from IRT. and rerun this cell.\")\n",
    "                \n",
    "            else:\n",
    "                print(\"Reading specialty difficulty from file.\")\n",
    "                # read specialty_difficulty.csv file\n",
    "                specialty_difficulty_irt = pd.read_csv(folder + '/'+ configurations[course_id]['logreg_year']+\"/result_logreg_\" + configurations[course_id]['logreg_model']+\"/specialty_difficulty.csv\")\n",
    "                attempt_counter_spec_irt= pd.read_csv(folder + '/'+ configurations[course_id]['logreg_year']+\"/processed\"+\"/attempt_counter_spec.csv\")\n",
    "                old_new_skill= listOfKC.copy()\n",
    "                # get specialty difficulty and specialty_difficulty_updates_list from get_question_difficulty_form_previous_year function\n",
    "                specialty_difficulty, specialty_difficulty_updates_list, attempt_counter_spec = get_spec_difficulty_form_previous_year(specialty_difficulty_irt, old_new_skill, attempt_counter_spec_irt, specialty_difficulty, specialty_difficulty_updates_list, attempt_counter_spec, count_attemp=False, reverse_coef=True)\n",
    "    else:\n",
    "        print(\"Learning specialty difficulty from ELO directly.\")\n",
    "        \n",
    "\n",
    "    # reshape the specialty_diffciulty and attempt_counter_spec arrays\n",
    "    specialty_difficulty = specialty_difficulty.reshape((1,len(specialties)))\n",
    "    attempt_counter_spec = attempt_counter_spec.reshape((1,len(specialties)))\n",
    "    \n",
    "# number of times a question on the defined topic is solved\n",
    "#attempt_counter_student_spec = pd.DataFrame(0, index=np.arange(uSize), columns=list(dict_of_kc.values()))\n",
    "\n",
    "#number of time a question was answered by each user\n",
    "#attempt_counter_question = pd.DataFrame(0, index=np.arange(qSize), columns=[\"n_attempt\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Training for learning item difficulty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.sort_values(by=[\"timestamp\", \"item_id\"], inplace=True) #first, timestamp should be converted to datetime\n",
    "train_set.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M-Elo execution for learning item difficulty is started.\n",
      "M-Elo execution for learning item difficulty is ended.\n"
     ]
    }
   ],
   "source": [
    "# ### Calling Elo function for train data to learn the difficulty of items ###\n",
    "## when doing training for learning item difficulty, don't forget to first sort values by their timestamp. \n",
    "warnings.filterwarnings(action='once') \n",
    "if configurations[course_id]['first_go']:\n",
    "    data_outputs = runelo_item_training(train_set,include_spec_difficulty=configurations[course_id]['include_spec_difficulty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.isdir(EXPERIMENT_FOLDER+\"/training_output_1go\"):\n",
    "    os.makedirs(EXPERIMENT_FOLDER+\"/training_output_1go\")\n",
    "student_ability_updates=pd.DataFrame(student_ability_updates_list)\n",
    "question_difficulty_updates=pd.DataFrame(question_difficulty_updates_list)\n",
    "student_ability_updates.to_csv(EXPERIMENT_FOLDER+\"/training_output_1go/student_ability_updates.csv\", index=True)\n",
    "question_difficulty_updates.to_csv(EXPERIMENT_FOLDER+\"/training_output_1go/question_difficulty_updates.csv\", index=True)\n",
    "question_difficulty_df = pd.DataFrame(question_difficulty)\n",
    "learner_competency_df = pd.DataFrame(learner_competency)\n",
    "question_difficulty_df.to_csv(EXPERIMENT_FOLDER+\"/training_output_1go/question_difficulty.csv\", index=True)\n",
    "learner_competency_df.to_csv(EXPERIMENT_FOLDER+\"/training_output_1go/learner_competency.csv\", index=True)\n",
    "attempt_counter_question_df = pd.DataFrame(attempt_counter_question)\n",
    "attempt_counter_question_df.to_csv(EXPERIMENT_FOLDER+\"/training_output_1go/attempt_counter_question.csv\", index=True)\n",
    "attempt_counter_student_spec_df = pd.DataFrame(attempt_counter_student_spec)\n",
    "attempt_counter_student_spec_df.to_csv(EXPERIMENT_FOLDER+\"/training_output_1go/attempt_counter_student_spec.csv\", index=True)\n",
    "\n",
    "# create a DataFrame from the dictionary\n",
    "data_outputs = pd.DataFrame(data_outputs)\n",
    "data_outputs.to_csv(EXPERIMENT_FOLDER+\"/training_output_1go/data_outputs.csv\", index=True)\n",
    "\n",
    "\n",
    "if include_spec_difficulty:\n",
    "    specialty_difficulty_updates_df=pd.DataFrame(specialty_difficulty_updates_list)\n",
    "    specialty_difficulty_updates_df.to_csv(EXPERIMENT_FOLDER+\"/training_output_1go/specialty_difficulty_updates.csv\", index=True)\n",
    "    specialty_difficulty_df = pd.DataFrame(specialty_difficulty)\n",
    "    specialty_difficulty_df.to_csv(EXPERIMENT_FOLDER+\"/training_output_1go/specialty_difficulty.csv\", index=True)\n",
    "    attempt_counter_spec_df = pd.DataFrame(attempt_counter_spec)\n",
    "    attempt_counter_spec_df.to_csv(EXPERIMENT_FOLDER+\"/training_output_1go/attempt_counter_spec.csv\", index=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Training for learning user competency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M-Elo execution for estimating student competency is started.\n",
      "M-Elo execution for estimating student competency is ended.\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(action='once')\n",
    "if configurations[course_id]['second_go']:\n",
    "    ### Calling Elo function for train data to learn competency of students ###\n",
    "    ## Re-initializing students parameters\n",
    "    learner_competency = np.zeros((uSize, tSize)) #stores student's profiency level on each topic and is updated upon each attempt\n",
    "    attempt_counter_student_spec=np.zeros((uSize, tSize))\n",
    "\n",
    "    unique_students = pre_processed_data['user_id'].unique()\n",
    "    specialties = list(dict_of_kc.values())\n",
    "    student_ability_updates_list = [{'student': uid, 'specialty': spec, 'ability': 0} for uid in unique_students for spec in specialties]\n",
    "\n",
    "    data_outputs = runelo_studentability_training(train_set,include_spec_difficulty=configurations[course_id]['include_spec_difficulty'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Save update tracking for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data\n",
    "if not os.path.isdir(EXPERIMENT_FOLDER+\"/training_output\"):\n",
    "    os.makedirs(EXPERIMENT_FOLDER+\"/training_output\")\n",
    "student_ability_updates=pd.DataFrame(student_ability_updates_list)\n",
    "question_difficulty_updates=pd.DataFrame(question_difficulty_updates_list)\n",
    "student_ability_updates.to_csv(EXPERIMENT_FOLDER+\"/training_output/student_ability_updates.csv\", index=True)\n",
    "question_difficulty_updates.to_csv(EXPERIMENT_FOLDER+\"/training_output/question_difficulty_updates.csv\", index=True)\n",
    "question_difficulty_df = pd.DataFrame(question_difficulty)\n",
    "learner_competency_df = pd.DataFrame(learner_competency)\n",
    "question_difficulty_df.to_csv(EXPERIMENT_FOLDER+\"/training_output/question_difficulty.csv\", index=True)\n",
    "learner_competency_df.to_csv(EXPERIMENT_FOLDER+\"/training_output/learner_competency.csv\", index=True)\n",
    "attempt_counter_student_spec_df = pd.DataFrame(attempt_counter_student_spec)\n",
    "attempt_counter_student_spec_df.to_csv(EXPERIMENT_FOLDER+\"/training_output/attempt_counter_student_spec.csv\", index=True)\n",
    "attempt_counter_question_df = pd.DataFrame(attempt_counter_question)\n",
    "attempt_counter_question_df.to_csv(EXPERIMENT_FOLDER+\"/training_output/attempt_counter_question.csv\", index=True)\n",
    "\n",
    "# create a DataFrame from the dictionary\n",
    "data_outputs = pd.DataFrame(data_outputs)\n",
    "data_outputs.to_csv(EXPERIMENT_FOLDER+\"/training_output/data_outputs.csv\", index=True)\n",
    "\n",
    "df = pd.DataFrame(listOfKC)\n",
    "df.to_csv(EXPERIMENT_FOLDER+\"/training_output/listOfKC.csv\", index=True)\n",
    "\n",
    "if include_spec_difficulty:\n",
    "    specialty_difficulty_updates_df=pd.DataFrame(specialty_difficulty_updates_list)\n",
    "    specialty_difficulty_updates_df.to_csv(EXPERIMENT_FOLDER+\"/training_output/specialty_difficulty_updates.csv\", index=True)\n",
    "    specialty_difficulty_df = pd.DataFrame(specialty_difficulty)\n",
    "    specialty_difficulty_df.to_csv(EXPERIMENT_FOLDER+\"/training_output/specialty_difficulty.csv\", index=True)\n",
    "    attempt_counter_spec_df = pd.DataFrame(attempt_counter_spec)\n",
    "    attempt_counter_spec_df.to_csv(EXPERIMENT_FOLDER+\"/training_output/attempt_counter_spec.csv\", index=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Test for learning item difficulty \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create temporary copie to be used in learning user rating (not affected by item training)\n",
    "learner_competency_tmp = learner_competency.copy()\n",
    "attempt_counter_student_spec_tmp = attempt_counter_student_spec.copy()\n",
    "student_ability_updates_tmp = student_ability_updates_list.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Calling Elo function for train data to learn the difficulty of items ###\n",
    "## when doing training for learning item difficulty, don't forget to first sort values by their timestamp. \n",
    "test_set.sort_values(by=[\"timestamp\", \"item_id\"], inplace=True) #first, timestamp should be converted to datetime\n",
    "test_set.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M-Elo execution for learning item difficulty is started.\n",
      "M-Elo execution for learning item difficulty is ended.\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(action='once')\n",
    "if configurations[course_id]['first_go']:\n",
    "        data_outputs = runelo_item_training(test_set,include_spec_difficulty=configurations[course_id]['include_spec_difficulty'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Test for learning user rating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M-Elo execution for estimating student competency is started.\n",
      "M-Elo execution for estimating student competency is ended.\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(action='once')\n",
    "if configurations[course_id]['second_go']:\n",
    "    learner_competency = learner_competency_tmp.copy()\n",
    "    attempt_counter_student_spec = attempt_counter_student_spec_tmp.copy()\n",
    "    student_ability_updates_list = student_ability_updates_tmp.copy()\n",
    "    data_outputs = runelo_studentability_training(test_set,include_spec_difficulty=configurations[course_id]['include_spec_difficulty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data and listOfKC\n",
    "\n",
    "# create output directory if it doesn't exist\n",
    "if not os.path.isdir(EXPERIMENT_FOLDER+\"/test_output\"):\n",
    "    os.makedirs(EXPERIMENT_FOLDER+\"/test_output\")\n",
    "data_outputs = pd.DataFrame(data_outputs)\n",
    "data_outputs.to_csv(EXPERIMENT_FOLDER+\"/test_output/data_outputs.csv\", index=True)\n",
    "question_difficulty_df = pd.DataFrame(question_difficulty)\n",
    "question_difficulty_df.to_csv(EXPERIMENT_FOLDER+\"/test_output/question_difficulty.csv\", index=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### AUC ACC and RMSE results for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE:  0.42011140529065305\n",
      "Test AUC:  0.8115394070176233\n",
      "Test ACC:  0.7326947176191655\n"
     ]
    }
   ],
   "source": [
    "rmse_test = CalculateRMSE(data_outputs[\"elo_ExpectedScore\"], data_outputs[\"actual_score\"], len(data_outputs[\"elo_ExpectedScore\"]))\n",
    "auc_test = auc_roc(data_outputs[\"actual_score\"], data_outputs[\"elo_ExpectedScore\"])\n",
    "acc_test = accuracy_score(np.array(data_outputs[\"actual_score\"]), np.array(data_outputs[\"elo_ExpectedScore\"]).round(), normalize=True)\n",
    "print(\"Test RMSE: \", rmse_test)\n",
    "print(\"Test AUC: \", auc_test)\n",
    "print(\"Test ACC: \", acc_test)\n",
    "\n",
    "# save the resut as txt file\n",
    "with open(os.path.join(EXPERIMENT_FOLDER, \"validation_results.txt\"), \"w\") as f:\n",
    "    f.write(\"Test RMSE: \" + str(rmse_test) + \"\\n\")\n",
    "    f.write(\"Test AUC: \" + str(auc_test) + \"\\n\")\n",
    "    f.write(\"Test ACC: \" + str(acc_test) + \"\\n\")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f029317bb4bfbc0fe13e73024792031a493f346c7d494ac5be7432f44fc5f10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
